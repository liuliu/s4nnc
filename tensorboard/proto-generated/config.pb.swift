// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: tensorboard/compat/proto/config.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

struct Tensorboard_GPUOptions: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Fraction of the available GPU memory to allocate for each process.
  /// 1 means to allocate all of the GPU memory, 0.5 means the process
  /// allocates up to ~50% of the available GPU memory.
  ///
  /// GPU memory is pre-allocated unless the allow_growth option is enabled.
  ///
  /// If greater than 1.0, uses CUDA unified memory to potentially oversubscribe
  /// the amount of memory available on the GPU device by using host memory as a
  /// swap space. Accessing memory not available on the device will be
  /// significantly slower as that would require memory transfer between the host
  /// and the device. Options to reduce the memory requirement should be
  /// considered before enabling this option as this may come with a negative
  /// performance impact. Oversubscription using the unified memory requires
  /// Pascal class or newer GPUs and it is currently only supported on the Linux
  /// operating system. See
  /// https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-requirements
  /// for the detailed requirements.
  var perProcessGpuMemoryFraction: Double {
    get {return _storage._perProcessGpuMemoryFraction}
    set {_uniqueStorage()._perProcessGpuMemoryFraction = newValue}
  }

  /// If true, the allocator does not pre-allocate the entire specified
  /// GPU memory region, instead starting small and growing as needed.
  var allowGrowth: Bool {
    get {return _storage._allowGrowth}
    set {_uniqueStorage()._allowGrowth = newValue}
  }

  /// The type of GPU allocation strategy to use.
  ///
  /// Allowed values:
  /// "": The empty string (default) uses a system-chosen default
  ///     which may change over time.
  ///
  /// "BFC": A "Best-fit with coalescing" algorithm, simplified from a
  ///        version of dlmalloc.
  var allocatorType: String {
    get {return _storage._allocatorType}
    set {_uniqueStorage()._allocatorType = newValue}
  }

  /// Delay deletion of up to this many bytes to reduce the number of
  /// interactions with gpu driver code.  If 0, the system chooses
  /// a reasonable default (several MBs).
  var deferredDeletionBytes: Int64 {
    get {return _storage._deferredDeletionBytes}
    set {_uniqueStorage()._deferredDeletionBytes = newValue}
  }

  /// A comma-separated list of GPU ids that determines the 'visible'
  /// to 'virtual' mapping of GPU devices.  For example, if TensorFlow
  /// can see 8 GPU devices in the process, and one wanted to map
  /// visible GPU devices 5 and 3 as "/device:GPU:0", and "/device:GPU:1",
  /// then one would specify this field as "5,3".  This field is similar in
  /// spirit to the CUDA_VISIBLE_DEVICES environment variable, except
  /// it applies to the visible GPU devices in the process.
  ///
  /// NOTE:
  /// 1. The GPU driver provides the process with the visible GPUs
  ///    in an order which is not guaranteed to have any correlation to
  ///    the *physical* GPU id in the machine.  This field is used for
  ///    remapping "visible" to "virtual", which means this operates only
  ///    after the process starts.  Users are required to use vendor
  ///    specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
  ///    physical to visible device mapping prior to invoking TensorFlow.
  /// 2. In the code, the ids in this list are also called "platform GPU id"s,
  ///    and the 'virtual' ids of GPU devices (i.e. the ids in the device
  ///    name "/device:GPU:<id>") are also called "TF GPU id"s. Please
  ///    refer to third_party/tensorflow/core/common_runtime/gpu/gpu_id.h
  ///    for more information.
  var visibleDeviceList: String {
    get {return _storage._visibleDeviceList}
    set {_uniqueStorage()._visibleDeviceList = newValue}
  }

  /// In the event polling loop sleep this many microseconds between
  /// PollEvents calls, when the queue is not empty.  If value is not
  /// set or set to 0, gets set to a non-zero default.
  var pollingActiveDelayUsecs: Int32 {
    get {return _storage._pollingActiveDelayUsecs}
    set {_uniqueStorage()._pollingActiveDelayUsecs = newValue}
  }

  /// This field is deprecated and ignored.
  var pollingInactiveDelayMsecs: Int32 {
    get {return _storage._pollingInactiveDelayMsecs}
    set {_uniqueStorage()._pollingInactiveDelayMsecs = newValue}
  }

  /// Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
  /// enabling this option forces all CPU tensors to be allocated with Cuda
  /// pinned memory. Normally, TensorFlow will infer which tensors should be
  /// allocated as the pinned memory. But in case where the inference is
  /// incomplete, this option can significantly speed up the cross-device memory
  /// copy performance as long as it fits the memory.
  /// Note that this option is not something that should be
  /// enabled by default for unknown or very large models, since all Cuda pinned
  /// memory is unpageable, having too much pinned memory might negatively impact
  /// the overall host system performance.
  var forceGpuCompatible: Bool {
    get {return _storage._forceGpuCompatible}
    set {_uniqueStorage()._forceGpuCompatible = newValue}
  }

  /// Everything inside experimental is subject to change and is not subject
  /// to API stability guarantees in
  /// https://www.tensorflow.org/guide/version_compat.
  var experimental: Tensorboard_GPUOptions.Experimental {
    get {return _storage._experimental ?? Tensorboard_GPUOptions.Experimental()}
    set {_uniqueStorage()._experimental = newValue}
  }
  /// Returns true if `experimental` has been explicitly set.
  var hasExperimental: Bool {return _storage._experimental != nil}
  /// Clears the value of `experimental`. Subsequent reads from it will return its default value.
  mutating func clearExperimental() {_uniqueStorage()._experimental = nil}

  var unknownFields = SwiftProtobuf.UnknownStorage()

  struct Experimental: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// The multi virtual device settings. If empty (not set), it will create
    /// single virtual device on each visible GPU, according to the settings
    /// in "visible_device_list" above. Otherwise, the number of elements in the
    /// list must be the same as the number of visible GPUs (after
    /// "visible_device_list" filtering if it is set), and the string represented
    /// device names (e.g. /device:GPU:<id>) will refer to the virtual
    /// devices and have the <id> field assigned sequentially starting from 0,
    /// according to the order they appear in this list and the "memory_limit"
    /// list inside each element. For example,
    ///   visible_device_list = "1,0"
    ///   virtual_devices { memory_limit: 1GB memory_limit: 2GB }
    ///   virtual_devices {}
    /// will create three virtual devices as:
    ///   /device:GPU:0 -> visible GPU 1 with 1GB memory
    ///   /device:GPU:1 -> visible GPU 1 with 2GB memory
    ///   /device:GPU:2 -> visible GPU 0 with all available memory
    ///
    /// NOTE:
    /// 1. It's invalid to set both this and "per_process_gpu_memory_fraction"
    ///    at the same time.
    /// 2. Currently this setting is per-process, not per-session. Using
    ///    different settings in different sessions within same process will
    ///    result in undefined behavior.
    var virtualDevices: [Tensorboard_GPUOptions.Experimental.VirtualDevices] = []

    /// If true, uses CUDA unified memory for memory allocations. If
    /// per_process_gpu_memory_fraction option is greater than 1.0, then unified
    /// memory is used regardless of the value for this field. See comments for
    /// per_process_gpu_memory_fraction field for more details and requirements
    /// of the unified memory. This option is useful to oversubscribe memory if
    /// multiple processes are sharing a single GPU while individually using less
    /// than 1.0 per process memory fraction.
    var useUnifiedMemory: Bool = false

    /// If > 1, the number of device-to-device copy streams to create
    /// for each GPUDevice.  Default value is 0, which is automatically
    /// converted to 1.
    var numDevToDevCopyStreams: Int32 = 0

    /// If non-empty, defines a good GPU ring order on a single worker based on
    /// device interconnect.  This assumes that all workers have the same GPU
    /// topology.  Specify as a comma-separated string, e.g. "3,2,1,0,7,6,5,4".
    /// This ring order is used by the RingReducer implementation of
    /// CollectiveReduce, and serves as an override to automatic ring order
    /// generation in OrderTaskDeviceMap() during CollectiveParam resolution.
    var collectiveRingOrder: String = String()

    /// If true then extra work is done by GPUDevice and GPUBFCAllocator to
    /// keep track of when GPU memory is freed and when kernels actually
    /// complete so that we can know when a nominally free memory chunk
    /// is really not subject to pending use.
    var timestampedAllocator: Bool = false

    /// Parameters for GPUKernelTracker.  By default no kernel tracking is done.
    /// Note that timestamped_allocator is only effective if some tracking is
    /// specified.
    ///
    /// If kernel_tracker_max_interval = n > 0, then a tracking event
    /// is inserted after every n kernels without an event.
    var kernelTrackerMaxInterval: Int32 = 0

    /// If kernel_tracker_max_bytes = n > 0, then a tracking event is
    /// inserted after every series of kernels allocating a sum of
    /// memory >= n.  If one kernel allocates b * n bytes, then one
    /// event will be inserted after it, but it will count as b against
    /// the pending limit.
    var kernelTrackerMaxBytes: Int32 = 0

    /// If kernel_tracker_max_pending > 0 then no more than this many
    /// tracking events can be outstanding at a time.  An attempt to
    /// launch an additional kernel will stall until an event
    /// completes.
    var kernelTrackerMaxPending: Int32 = 0

    /// BFC Allocator can return an allocated chunk of memory upto 2x the
    /// requested size. For virtual devices with tight memory constraints, and
    /// proportionately large allocation requests, this can lead to a significant
    /// reduction in available memory. The threshold below controls when a chunk
    /// should be split if the chunk size exceeds requested memory size. It is
    /// expressed as a fraction of total available memory for the tf device. For
    /// example setting it to 0.05 would imply a chunk needs to be split if its
    /// size exceeds the requested memory by 5% of the total virtual device/gpu
    /// memory size.
    var internalFragmentationFraction: Double = 0

    /// When true, use CUDA cudaMallocAsync API instead of TF gpu allocator.
    var useCudaMallocAsync: Bool = false

    /// By default, BFCAllocator may sleep when it runs out of memory, in the
    /// hopes that another thread will free up memory in the meantime.  Setting
    /// this to true disables the sleep; instead we'll OOM immediately.
    var disallowRetryOnAllocationFailure: Bool = false

    var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Configuration for breaking down a visible GPU into multiple "virtual"
    /// devices.
    struct VirtualDevices: Sendable {
      // SwiftProtobuf.Message conformance is added in an extension below. See the
      // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
      // methods supported on all messages.

      /// Per "virtual" device memory limit, in MB. The number of elements in
      /// the list is the number of virtual devices to create on the
      /// corresponding visible GPU (see "virtual_devices" below).
      /// If empty, it will create single virtual device taking all available
      /// memory from the device.
      ///
      /// For the concept of "visible" and "virtual" GPU, see the comments for
      /// "visible_device_list" above for more information.
      var memoryLimitMb: [Float] = []

      /// Priority values to use with the virtual devices. Use the cuda function
      /// cudaDeviceGetStreamPriorityRange to query for valid range of values for
      /// priority.
      ///
      /// On a P4000 GPU with cuda 10.1, the priority range reported was 0 for
      /// least priority and -1 for greatest priority.
      ///
      /// If this field is not specified, then the virtual devices will be
      /// created with the default. If this field has values set, then the size
      /// of this must match with the above memory_limit_mb.
      var priority: [Int32] = []

      var unknownFields = SwiftProtobuf.UnknownStorage()

      init() {}
    }

    init() {}
  }

  init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Options passed to the graph optimizer
struct Tensorboard_OptimizerOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// If true, optimize the graph using common subexpression elimination.
  /// Note: the optimization Level L1 will override this setting to true. So in
  /// order to disable common subexpression elimination the opt_level has to be
  /// set to L0.
  var doCommonSubexpressionElimination: Bool = false

  /// If true, perform constant folding optimization on the graph.
  /// Note: the optimization Level L1 will override this setting to true. So in
  /// order to disable constant folding the opt_level has to be set to L0.
  var doConstantFolding: Bool = false

  /// Constant folding optimization replaces tensors whose values can be
  /// predetermined, with constant nodes. To avoid inserting too large constants,
  /// the size of each constant created can be limited. If this value is zero, a
  /// default limit of 10 MiB will be applied. If constant folding optimization
  /// is disabled, this value is ignored.
  var maxFoldedConstantInBytes: Int64 = 0

  /// If true, perform function inlining on the graph.
  var doFunctionInlining: Bool = false

  /// Overall optimization level. The actual optimizations applied will be the
  /// logical OR of the flags that this level implies and any flags already set.
  var optLevel: Tensorboard_OptimizerOptions.Level = .l1

  var globalJitLevel: Tensorboard_OptimizerOptions.GlobalJitLevel = .default

  /// CPU code will be autoclustered only if global_jit_level >= ON_1 and either:
  ///  - this flag is true, or
  ///  - TF_XLA_FLAGS contains --tf_xla_cpu_global_jit=true.
  var cpuGlobalJit: Bool = false

  var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Optimization level
  enum Level: SwiftProtobuf.Enum, Swift.CaseIterable {
    typealias RawValue = Int

    /// L1 is the default level.
    /// Optimization performed at L1 :
    /// 1. Common subexpression elimination
    /// 2. Constant folding
    case l1 // = 0

    /// No optimizations
    case l0 // = -1
    case UNRECOGNIZED(Int)

    init() {
      self = .l1
    }

    init?(rawValue: Int) {
      switch rawValue {
      case -1: self = .l0
      case 0: self = .l1
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    var rawValue: Int {
      switch self {
      case .l0: return -1
      case .l1: return 0
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    static let allCases: [Tensorboard_OptimizerOptions.Level] = [
      .l1,
      .l0,
    ]

  }

  /// Control the use of the compiler/jit.  Experimental.
  enum GlobalJitLevel: SwiftProtobuf.Enum, Swift.CaseIterable {
    typealias RawValue = Int

    /// Default setting ("off" now, but later expected to be "on")
    case `default` // = 0
    case off // = -1

    /// The following settings turn on compilation, with higher values being
    /// more aggressive.  Higher values may reduce opportunities for parallelism
    /// and may use more memory.  (At present, there is no distinction, but this
    /// is expected to change.)
    case on1 // = 1
    case on2 // = 2
    case UNRECOGNIZED(Int)

    init() {
      self = .default
    }

    init?(rawValue: Int) {
      switch rawValue {
      case -1: self = .off
      case 0: self = .default
      case 1: self = .on1
      case 2: self = .on2
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    var rawValue: Int {
      switch self {
      case .off: return -1
      case .default: return 0
      case .on1: return 1
      case .on2: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    static let allCases: [Tensorboard_OptimizerOptions.GlobalJitLevel] = [
      .default,
      .off,
      .on1,
      .on2,
    ]

  }

  init() {}
}

struct Tensorboard_GraphOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// If true, use control flow to schedule the activation of Recv nodes.
  /// (Currently ignored.)
  var enableRecvScheduling: Bool = false

  /// Options controlling how graph is optimized.
  var optimizerOptions: Tensorboard_OptimizerOptions {
    get {return _optimizerOptions ?? Tensorboard_OptimizerOptions()}
    set {_optimizerOptions = newValue}
  }
  /// Returns true if `optimizerOptions` has been explicitly set.
  var hasOptimizerOptions: Bool {return self._optimizerOptions != nil}
  /// Clears the value of `optimizerOptions`. Subsequent reads from it will return its default value.
  mutating func clearOptimizerOptions() {self._optimizerOptions = nil}

  /// The number of steps to run before returning a cost model detailing
  /// the memory usage and performance of each node of the graph. 0 means
  /// no cost model.
  var buildCostModel: Int64 = 0

  /// The number of steps to skip before collecting statistics for the
  /// cost model.
  var buildCostModelAfter: Int64 = 0

  /// Annotate each Node with Op output shape data, to the extent it can
  /// be statically inferred.
  var inferShapes: Bool = false

  /// Only place the subgraphs that are run, rather than the entire graph.
  ///
  /// This is useful for interactive graph building, where one might
  /// produce graphs that cannot be placed during the debugging
  /// process.  In particular, it allows the client to continue work in
  /// a session after adding a node to a graph whose placement
  /// constraints are unsatisfiable.
  var placePrunedGraph: Bool = false

  /// If true, transfer float values between processes as bfloat16.
  var enableBfloat16Sendrecv: Bool = false

  /// If > 0, record a timeline every this many steps.
  /// EXPERIMENTAL: This currently has no effect in MasterSession.
  var timelineStep: Int32 = 0

  /// Options that control the type and amount of graph rewriting.
  /// Not currently configurable via the public Python API (i.e. there is no API
  /// stability guarantee if you import RewriterConfig explicitly).
  var rewriteOptions: Tensorboard_RewriterConfig {
    get {return _rewriteOptions ?? Tensorboard_RewriterConfig()}
    set {_rewriteOptions = newValue}
  }
  /// Returns true if `rewriteOptions` has been explicitly set.
  var hasRewriteOptions: Bool {return self._rewriteOptions != nil}
  /// Clears the value of `rewriteOptions`. Subsequent reads from it will return its default value.
  mutating func clearRewriteOptions() {self._rewriteOptions = nil}

  var unknownFields = SwiftProtobuf.UnknownStorage()

  init() {}

  fileprivate var _optimizerOptions: Tensorboard_OptimizerOptions? = nil
  fileprivate var _rewriteOptions: Tensorboard_RewriterConfig? = nil
}

struct Tensorboard_ThreadPoolOptionProto: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// The number of threads in the pool.
  ///
  /// 0 means the system picks a value based on where this option proto is used
  /// (see the declaration of the specific field for more info).
  var numThreads: Int32 = 0

  /// The global name of the threadpool.
  ///
  /// If empty, then the threadpool is made and used according to the scope it's
  /// in - e.g., for a session threadpool, it is used by that session only.
  ///
  /// If non-empty, then:
  /// - a global threadpool associated with this name is looked
  ///   up or created. This allows, for example, sharing one threadpool across
  ///   many sessions (e.g., like the default behavior, if
  ///   inter_op_parallelism_threads is not configured), but still partitioning
  ///   into a large and small pool.
  /// - if the threadpool for this global_name already exists, then it is an
  ///   error if the existing pool was created using a different num_threads
  ///   value as is specified on this call.
  /// - threadpools created this way are never garbage collected.
  var globalName: String = String()

  var unknownFields = SwiftProtobuf.UnknownStorage()

  init() {}
}

struct Tensorboard_RPCOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// If true, always use RPC to contact the session target.
  ///
  /// If false (the default option), TensorFlow may use an optimized
  /// transport for client-master communication that avoids the RPC
  /// stack. This option is primarily for used testing the RPC stack.
  var useRpcForInprocessMaster: Bool = false

  /// The compression algorithm to be used. One of "deflate", "gzip".
  var compressionAlgorithm: String = String()

  /// If compression_algorithm is set, the compression level to be used.
  /// From 0 (no compression), up to 3.
  var compressionLevel: Int32 = 0

  /// Setting cache_rpc_response to true will enable sender side caching of
  /// response for RecvTensorAsync and RecvBufAsync to allow receiver to retry
  /// requests . This is only necessary when the network fabric is experiencing a
  /// significant error rate.  Without it we'll fail a step on an network error,
  /// while with it we'll be able to complete long steps (like complex
  /// initializations) in the face of some network errors during RecvTensor.
  var cacheRpcResponse: Bool = false

  /// Disables TCP connection sharing when opening a new RPC channel.
  var disableSessionConnectionSharing: Bool = false

  /// Setting num_channels_per_target > 0 allows uses of multiple channels to
  /// communicate to the same target. This can be used to improve the aggregate
  /// throughput on high speed links (e.g 100G) where single connection is not
  /// sufficient to maximize link utilization. Note that a single RPC only goes
  /// on a single channel, this only helps in situations where there are multiple
  /// transfers to the same target overlapping in time.
  var numChannelsPerTarget: Int32 = 0

  var unknownFields = SwiftProtobuf.UnknownStorage()

  init() {}
}

/// Metadata about the session.
///
/// This can be used by the runtime and the Ops for debugging, monitoring, etc.
///
/// The (name, version) tuple is expected to be a unique identifier for
/// sessions within the same process.
///
/// NOTE: This is currently used and propagated only by the direct session.
struct Tensorboard_SessionMetadata: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  var name: String = String()

  /// The version is optional. If set, needs to be >= 0.
  var version: Int64 = 0

  var unknownFields = SwiftProtobuf.UnknownStorage()

  init() {}
}

/// Session configuration parameters.
/// The system picks appropriate values for fields that are not set.
struct Tensorboard_ConfigProto: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Map from device type name (e.g., "CPU" or "GPU" ) to maximum
  /// number of devices of that type to use.  If a particular device
  /// type is not found in the map, the system picks an appropriate
  /// number.
  var deviceCount: Dictionary<String,Int32> {
    get {return _storage._deviceCount}
    set {_uniqueStorage()._deviceCount = newValue}
  }

  /// The execution of an individual op (for some op types) can be
  /// parallelized on a pool of intra_op_parallelism_threads.
  /// 0 means the system picks an appropriate number.
  ///
  /// If you create an ordinary session, e.g., from Python or C++,
  /// then there is exactly one intra op thread pool per process.
  /// The first session created determines the number of threads in this pool.
  /// All subsequent sessions reuse/share this one global pool.
  ///
  /// There are notable exceptions to the default behavior described above:
  /// 1. There is an environment variable  for overriding this thread pool,
  ///    named TF_OVERRIDE_GLOBAL_THREADPOOL.
  /// 2. When connecting to a server, such as a remote `tf.train.Server`
  ///    instance, then this option will be ignored altogether.
  var intraOpParallelismThreads: Int32 {
    get {return _storage._intraOpParallelismThreads}
    set {_uniqueStorage()._intraOpParallelismThreads = newValue}
  }

  /// Nodes that perform blocking operations are enqueued on a pool of
  /// inter_op_parallelism_threads available in each process.
  ///
  /// 0 means the system picks an appropriate number.
  /// Negative means all operations are performed in caller's thread.
  ///
  /// Note that the first Session created in the process sets the
  /// number of threads for all future sessions unless use_per_session_threads is
  /// true or session_inter_op_thread_pool is configured.
  var interOpParallelismThreads: Int32 {
    get {return _storage._interOpParallelismThreads}
    set {_uniqueStorage()._interOpParallelismThreads = newValue}
  }

  /// If true, use a new set of threads for this session rather than the global
  /// pool of threads. Only supported by direct sessions.
  ///
  /// If false, use the global threads created by the first session, or the
  /// per-session thread pools configured by session_inter_op_thread_pool.
  ///
  /// This option is deprecated. The same effect can be achieved by setting
  /// session_inter_op_thread_pool to have one element, whose num_threads equals
  /// inter_op_parallelism_threads.
  var usePerSessionThreads: Bool {
    get {return _storage._usePerSessionThreads}
    set {_uniqueStorage()._usePerSessionThreads = newValue}
  }

  /// This option is experimental - it may be replaced with a different mechanism
  /// in the future.
  ///
  /// Configures session thread pools. If this is configured, then RunOptions for
  /// a Run call can select the thread pool to use.
  ///
  /// The intended use is for when some session invocations need to run in a
  /// background pool limited to a small number of threads:
  /// - For example, a session may be configured to have one large pool (for
  /// regular compute) and one small pool (for periodic, low priority work);
  /// using the small pool is currently the mechanism for limiting the inter-op
  /// parallelism of the low priority work.  Note that it does not limit the
  /// parallelism of work spawned by a single op kernel implementation.
  /// - Using this setting is normally not needed in training, but may help some
  /// serving use cases.
  /// - It is also generally recommended to set the global_name field of this
  /// proto, to avoid creating multiple large pools. It is typically better to
  /// run the non-low-priority work, even across sessions, in a single large
  /// pool.
  var sessionInterOpThreadPool: [Tensorboard_ThreadPoolOptionProto] {
    get {return _storage._sessionInterOpThreadPool}
    set {_uniqueStorage()._sessionInterOpThreadPool = newValue}
  }

  /// Assignment of Nodes to Devices is recomputed every placement_period
  /// steps until the system warms up (at which point the recomputation
  /// typically slows down automatically).
  var placementPeriod: Int32 {
    get {return _storage._placementPeriod}
    set {_uniqueStorage()._placementPeriod = newValue}
  }

  /// When any filters are present sessions will ignore all devices which do not
  /// match the filters. Each filter can be partially specified, e.g. "/job:ps"
  /// "/job:worker/replica:3", etc.
  var deviceFilters: [String] {
    get {return _storage._deviceFilters}
    set {_uniqueStorage()._deviceFilters = newValue}
  }

  /// Options that apply to all GPUs.
  var gpuOptions: Tensorboard_GPUOptions {
    get {return _storage._gpuOptions ?? Tensorboard_GPUOptions()}
    set {_uniqueStorage()._gpuOptions = newValue}
  }
  /// Returns true if `gpuOptions` has been explicitly set.
  var hasGpuOptions: Bool {return _storage._gpuOptions != nil}
  /// Clears the value of `gpuOptions`. Subsequent reads from it will return its default value.
  mutating func clearGpuOptions() {_uniqueStorage()._gpuOptions = nil}

  /// Whether soft placement is allowed. If allow_soft_placement is true,
  /// an op will be placed on CPU if
  ///   1. there's no GPU implementation for the OP
  /// or
  ///   2. no GPU devices are known or registered
  /// or
  ///   3. need to co-locate with reftype input(s) which are from CPU.
  var allowSoftPlacement: Bool {
    get {return _storage._allowSoftPlacement}
    set {_uniqueStorage()._allowSoftPlacement = newValue}
  }

  /// Whether device placements should be logged.
  var logDevicePlacement: Bool {
    get {return _storage._logDevicePlacement}
    set {_uniqueStorage()._logDevicePlacement = newValue}
  }

  /// Options that apply to all graphs.
  var graphOptions: Tensorboard_GraphOptions {
    get {return _storage._graphOptions ?? Tensorboard_GraphOptions()}
    set {_uniqueStorage()._graphOptions = newValue}
  }
  /// Returns true if `graphOptions` has been explicitly set.
  var hasGraphOptions: Bool {return _storage._graphOptions != nil}
  /// Clears the value of `graphOptions`. Subsequent reads from it will return its default value.
  mutating func clearGraphOptions() {_uniqueStorage()._graphOptions = nil}

  /// Global timeout for all blocking operations in this session.  If non-zero,
  /// and not overridden on a per-operation basis, this value will be used as the
  /// deadline for all blocking operations.
  var operationTimeoutInMs: Int64 {
    get {return _storage._operationTimeoutInMs}
    set {_uniqueStorage()._operationTimeoutInMs = newValue}
  }

  /// Options that apply when this session uses the distributed runtime.
  var rpcOptions: Tensorboard_RPCOptions {
    get {return _storage._rpcOptions ?? Tensorboard_RPCOptions()}
    set {_uniqueStorage()._rpcOptions = newValue}
  }
  /// Returns true if `rpcOptions` has been explicitly set.
  var hasRpcOptions: Bool {return _storage._rpcOptions != nil}
  /// Clears the value of `rpcOptions`. Subsequent reads from it will return its default value.
  mutating func clearRpcOptions() {_uniqueStorage()._rpcOptions = nil}

  /// Optional list of all workers to use in this session.
  var clusterDef: Tensorboard_ClusterDef {
    get {return _storage._clusterDef ?? Tensorboard_ClusterDef()}
    set {_uniqueStorage()._clusterDef = newValue}
  }
  /// Returns true if `clusterDef` has been explicitly set.
  var hasClusterDef: Bool {return _storage._clusterDef != nil}
  /// Clears the value of `clusterDef`. Subsequent reads from it will return its default value.
  mutating func clearClusterDef() {_uniqueStorage()._clusterDef = nil}

  /// If true, any resources such as Variables used in the session will not be
  /// shared with other sessions. However, when clusterspec propagation is
  /// enabled, this field is ignored and sessions are always isolated.
  var isolateSessionState: Bool {
    get {return _storage._isolateSessionState}
    set {_uniqueStorage()._isolateSessionState = newValue}
  }

  /// When true, WorkerSessions are created with device attributes from the
  /// full cluster.
  /// This is helpful when a worker wants to partition a graph
  /// (for example during a PartitionedCallOp).
  var shareClusterDevicesInSession: Bool {
    get {return _storage._shareClusterDevicesInSession}
    set {_uniqueStorage()._shareClusterDevicesInSession = newValue}
  }

  var experimental: Tensorboard_ConfigProto.Experimental {
    get {return _storage._experimental ?? Tensorboard_ConfigProto.Experimental()}
    set {_uniqueStorage()._experimental = newValue}
  }
  /// Returns true if `experimental` has been explicitly set.
  var hasExperimental: Bool {return _storage._experimental != nil}
  /// Clears the value of `experimental`. Subsequent reads from it will return its default value.
  mutating func clearExperimental() {_uniqueStorage()._experimental = nil}

  var unknownFields = SwiftProtobuf.UnknownStorage()

  /// Everything inside Experimental is subject to change and is not subject
  /// to API stability guarantees in
  /// https://www.tensorflow.org/guide/version_compat.
  struct Experimental: @unchecked Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// Task name for group resolution.
    var collectiveGroupLeader: String {
      get {return _storage._collectiveGroupLeader}
      set {_uniqueStorage()._collectiveGroupLeader = newValue}
    }

    /// Which executor to use, the default executor will be used
    /// if it is an empty string or "DEFAULT"
    var executorType: String {
      get {return _storage._executorType}
      set {_uniqueStorage()._executorType = newValue}
    }

    /// Guidance to formatting of large RecvBuf fields for transfer.
    /// Any positive value sets the max chunk size.  0 defaults to 4096.
    /// Any negative value indicates no max, i.e. one chunk only.
    var recvBufMaxChunk: Int32 {
      get {return _storage._recvBufMaxChunk}
      set {_uniqueStorage()._recvBufMaxChunk = newValue}
    }

    /// If true, and supported by the platform, the runtime will attempt to
    /// use NUMA affinity where applicable.  One consequence will be the
    /// existence of as many CPU devices as there are available NUMA nodes.
    var useNumaAffinity: Bool {
      get {return _storage._useNumaAffinity}
      set {_uniqueStorage()._useNumaAffinity = newValue}
    }

    /// If true, make collective op execution order sequential and deterministic
    /// for potentially concurrent collective instances.
    var collectiveDeterministicSequentialExecution: Bool {
      get {return _storage._collectiveDeterministicSequentialExecution}
      set {_uniqueStorage()._collectiveDeterministicSequentialExecution = newValue}
    }

    /// If true, use NCCL for CollectiveOps.  This feature is highly
    /// experimental.
    var collectiveNccl: Bool {
      get {return _storage._collectiveNccl}
      set {_uniqueStorage()._collectiveNccl = newValue}
    }

    /// In the following, session state means the value of a variable, elements
    /// in a hash table, or any other resource, accessible by worker sessions
    /// held by a TF server.
    ///
    /// When ClusterSpec propagation is enabled, the value of
    /// isolate_session_state is ignored when deciding whether to share session
    /// states in a TF server (for backwards compatibility reasons).
    /// - If share_session_state_in_clusterspec_propagation is true, the session
    /// states are shared.
    /// - If share_session_state_in_clusterspec_propagation is false, session
    /// states are isolated.
    ///
    /// When clusterspec propagation is not used, the value of
    /// share_session_state_in_clusterspec_propagation is ignored when deciding
    /// whether to share session states in a TF server.
    /// - If isolate_session_state is true, session states are isolated.
    /// - If isolate_session_state is false, session states are shared.
    ///
    /// TODO(b/129330037): Add a single API that consistently treats
    /// isolate_session_state and ClusterSpec propagation.
    var shareSessionStateInClusterspecPropagation: Bool {
      get {return _storage._shareSessionStateInClusterspecPropagation}
      set {_uniqueStorage()._shareSessionStateInClusterspecPropagation = newValue}
    }

    /// If using a direct session, disable spinning while waiting for work in
    /// the thread pool. This may result in higher latency for completing ops,
    /// but in the case where there is a lot of spinning may result in lower
    /// CPU usage.
    var disableThreadSpinning: Bool {
      get {return _storage._disableThreadSpinning}
      set {_uniqueStorage()._disableThreadSpinning = newValue}
    }

    /// This was promoted to a non-experimental API. Please use
    /// ConfigProto.share_cluster_devices_in_session instead.
    var shareClusterDevicesInSession: Bool {
      get {return _storage._shareClusterDevicesInSession}
      set {_uniqueStorage()._shareClusterDevicesInSession = newValue}
    }

    /// Metadata about the session.
    ///
    /// If set, this can be used by the runtime and the Ops for debugging,
    /// monitoring, etc.
    ///
    /// NOTE: This is currently used and propagated only by the direct session.
    var sessionMetadata: Tensorboard_SessionMetadata {
      get {return _storage._sessionMetadata ?? Tensorboard_SessionMetadata()}
      set {_uniqueStorage()._sessionMetadata = newValue}
    }
    /// Returns true if `sessionMetadata` has been explicitly set.
    var hasSessionMetadata: Bool {return _storage._sessionMetadata != nil}
    /// Clears the value of `sessionMetadata`. Subsequent reads from it will return its default value.
    mutating func clearSessionMetadata() {_uniqueStorage()._sessionMetadata = nil}

    /// If true, the session may treat the graph as being static for optimization
    /// purposes.
    ///
    /// If this option is set to true when a session is created, the full
    /// GraphDef must be passed in a single call to Session::Create(), and
    /// Session::Extend() may not be supported.
    var optimizeForStaticGraph: Bool {
      get {return _storage._optimizeForStaticGraph}
      set {_uniqueStorage()._optimizeForStaticGraph = newValue}
    }

    /// This field will eventually be deprecated and replaced by
    /// mlir_bridge_rollout (b/166038521).
    ///
    /// Whether to enable the MLIR-based TF->XLA bridge.
    ///
    /// This is a replacement to the existing bridge, and not ready for
    /// production usage yet.
    /// If this option is set to true when a session is created, MLIR is used to
    /// perform the set of graph transformations to put the graph in a form that
    /// can be executed with delegation of some computations to an accelerator.
    /// This builds on the model of XLA where a subset of the graph is
    /// encapsulated and attached to a "compile" operation, whose result is fed
    /// to an "execute" operation. The kernel for these operations is responsible
    /// to lower the encapsulated graph to a particular device.
    var enableMlirBridge: Bool {
      get {return _storage._enableMlirBridge}
      set {_uniqueStorage()._enableMlirBridge = newValue}
    }

    /// This field is underdevelopment, for now use enable_mlir_bridge
    /// (b/166038521).
    ///
    /// Whether to enable the MLIR-based TF->XLA bridge.
    var mlirBridgeRollout: Tensorboard_ConfigProto.Experimental.MlirBridgeRollout {
      get {return _storage._mlirBridgeRollout}
      set {_uniqueStorage()._mlirBridgeRollout = newValue}
    }

    /// Whether to enable the MLIR-based Graph optimizations.
    ///
    /// This will become a part of standard Tensorflow graph optimization
    /// pipeline, currently this is only used for gradual migration and testing
    /// new passes that are replacing existing optimizations in Grappler.
    var enableMlirGraphOptimization: Bool {
      get {return _storage._enableMlirGraphOptimization}
      set {_uniqueStorage()._enableMlirGraphOptimization = newValue}
    }

    /// If true, the session will not store an additional copy of the graph for
    /// each subgraph.
    ///
    /// If this option is set to true when a session is created, the
    /// `RunOptions.output_partition_graphs` options must not be set.
    var disableOutputPartitionGraphs: Bool {
      get {return _storage._disableOutputPartitionGraphs}
      set {_uniqueStorage()._disableOutputPartitionGraphs = newValue}
    }

    /// Minimum number of batches run through the XLA graph before XLA fusion
    /// autotuner is enabled. Default value of zero disables the autotuner.
    ///
    /// The XLA fusion autotuner can improve performance by executing a heuristic
    /// search on the compiler parameters.
    var xlaFusionAutotunerThresh: Int64 {
      get {return _storage._xlaFusionAutotunerThresh}
      set {_uniqueStorage()._xlaFusionAutotunerThresh = newValue}
    }

    /// Whether runtime execution uses TFRT.
    var useTfrt: Bool {
      get {return _storage._useTfrt}
      set {_uniqueStorage()._useTfrt = newValue}
    }

    /// Whether functional control flow op lowering should be disabled. This is
    /// useful when executing within a portable runtime where control flow op
    /// kernels may not be loaded due to selective registration.
    var disableFunctionalOpsLowering: Bool {
      get {return _storage._disableFunctionalOpsLowering}
      set {_uniqueStorage()._disableFunctionalOpsLowering = newValue}
    }

    /// Provides a hint to XLA auto clustering to prefer forming a single large
    /// cluster that encompases most of the graph.
    var xlaPreferSingleGraphCluster: Bool {
      get {return _storage._xlaPreferSingleGraphCluster}
      set {_uniqueStorage()._xlaPreferSingleGraphCluster = newValue}
    }

    /// Distributed coordination service configurations.
    var coordinationConfig: Tensorboard_CoordinationServiceConfig {
      get {return _storage._coordinationConfig ?? Tensorboard_CoordinationServiceConfig()}
      set {_uniqueStorage()._coordinationConfig = newValue}
    }
    /// Returns true if `coordinationConfig` has been explicitly set.
    var hasCoordinationConfig: Bool {return _storage._coordinationConfig != nil}
    /// Clears the value of `coordinationConfig`. Subsequent reads from it will return its default value.
    mutating func clearCoordinationConfig() {_uniqueStorage()._coordinationConfig = nil}

    var unknownFields = SwiftProtobuf.UnknownStorage()

    /// An enum that describes the state of the MLIR bridge rollout.
    enum MlirBridgeRollout: SwiftProtobuf.Enum, Swift.CaseIterable {
      typealias RawValue = Int

      /// If this field is left unspecified, the MLIR bridge may be selectively
      /// enabled on a per graph basis.
      case unspecified // = 0

      /// Enabling the MLIR bridge enables it for all graphs in this session.
      case enabled // = 1

      /// Disabling the MLIR bridge disables it for all graphs in this session.
      case disabled // = 2

      /// Enable the MLIR bridge on a per graph basis based on an analysis of
      /// the features used in the graph. If the features used by the graph are
      /// supported by the MLIR bridge, the MLIR bridge will be used to run the
      /// graph.
      case safeModeEnabled // = 3

      /// Enable the MLIR bridge in a fallback mode on a per graph basis based
      /// on an analysis of the features used in the graph.
      /// Running the MLIR bridge in the fallback mode means that it is
      /// executed and it commits all the changes to the TF graph in case
      /// of success. And it does not in case of failures and let the old bridge
      /// to process the TF graph.
      case safeModeFallbackEnabled // = 4
      case UNRECOGNIZED(Int)

      init() {
        self = .unspecified
      }

      init?(rawValue: Int) {
        switch rawValue {
        case 0: self = .unspecified
        case 1: self = .enabled
        case 2: self = .disabled
        case 3: self = .safeModeEnabled
        case 4: self = .safeModeFallbackEnabled
        default: self = .UNRECOGNIZED(rawValue)
        }
      }

      var rawValue: Int {
        switch self {
        case .unspecified: return 0
        case .enabled: return 1
        case .disabled: return 2
        case .safeModeEnabled: return 3
        case .safeModeFallbackEnabled: return 4
        case .UNRECOGNIZED(let i): return i
        }
      }

      // The compiler won't synthesize support with the UNRECOGNIZED case.
      static let allCases: [Tensorboard_ConfigProto.Experimental.MlirBridgeRollout] = [
        .unspecified,
        .enabled,
        .disabled,
        .safeModeEnabled,
        .safeModeFallbackEnabled,
      ]

    }

    init() {}

    fileprivate var _storage = _StorageClass.defaultInstance
  }

  init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

/// Options for a single Run() call.
struct Tensorboard_RunOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  var traceLevel: Tensorboard_RunOptions.TraceLevel = .noTrace

  /// Time to wait for operation to complete in milliseconds.
  var timeoutInMs: Int64 = 0

  /// The thread pool to use, if session_inter_op_thread_pool is configured.
  /// To use the caller thread set this to -1 - this uses the caller thread
  /// to execute Session::Run() and thus avoids a context switch. Using the
  /// caller thread to execute Session::Run() should be done ONLY for simple
  /// graphs, where the overhead of an additional context switch is
  /// comparable with the overhead of Session::Run().
  var interOpThreadPool: Int32 = 0

  /// Whether the partition graph(s) executed by the executor(s) should be
  /// outputted via RunMetadata.
  var outputPartitionGraphs: Bool = false

  /// EXPERIMENTAL.  Options used to initialize DebuggerState, if enabled.
  var debugOptions: Tensorboard_DebugOptions {
    get {return _debugOptions ?? Tensorboard_DebugOptions()}
    set {_debugOptions = newValue}
  }
  /// Returns true if `debugOptions` has been explicitly set.
  var hasDebugOptions: Bool {return self._debugOptions != nil}
  /// Clears the value of `debugOptions`. Subsequent reads from it will return its default value.
  mutating func clearDebugOptions() {self._debugOptions = nil}

  /// When enabled, causes tensor allocation information to be included in
  /// the error message when the Run() call fails because the allocator ran
  /// out of memory (OOM).
  ///
  /// Enabling this option can slow down the Run() call.
  var reportTensorAllocationsUponOom: Bool = false

  var experimental: Tensorboard_RunOptions.Experimental {
    get {return _experimental ?? Tensorboard_RunOptions.Experimental()}
    set {_experimental = newValue}
  }
  /// Returns true if `experimental` has been explicitly set.
  var hasExperimental: Bool {return self._experimental != nil}
  /// Clears the value of `experimental`. Subsequent reads from it will return its default value.
  mutating func clearExperimental() {self._experimental = nil}

  var unknownFields = SwiftProtobuf.UnknownStorage()

  /// TODO(pbar) Turn this into a TraceOptions proto which allows
  /// tracing to be controlled in a more orthogonal manner?
  enum TraceLevel: SwiftProtobuf.Enum, Swift.CaseIterable {
    typealias RawValue = Int
    case noTrace // = 0
    case softwareTrace // = 1
    case hardwareTrace // = 2
    case fullTrace // = 3
    case UNRECOGNIZED(Int)

    init() {
      self = .noTrace
    }

    init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .noTrace
      case 1: self = .softwareTrace
      case 2: self = .hardwareTrace
      case 3: self = .fullTrace
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    var rawValue: Int {
      switch self {
      case .noTrace: return 0
      case .softwareTrace: return 1
      case .hardwareTrace: return 2
      case .fullTrace: return 3
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    static let allCases: [Tensorboard_RunOptions.TraceLevel] = [
      .noTrace,
      .softwareTrace,
      .hardwareTrace,
      .fullTrace,
    ]

  }

  /// Everything inside Experimental is subject to change and is not subject
  /// to API stability guarantees in
  /// https://www.tensorflow.org/guide/version_compat.
  struct Experimental: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// If non-zero, declares that this graph is going to use collective
    /// ops and must synchronize step_ids with any other graph with this
    /// same group_key value (in a distributed computation where tasks
    /// run disjoint graphs).
    var collectiveGraphKey: Int64 = 0

    /// If true, then operations (using the inter-op pool) across all
    /// session::run() calls will be centrally scheduled, optimizing for (median
    /// and tail) latency.
    /// Consider using this option for CPU-bound workloads like inference.
    var useRunHandlerPool: Bool = false

    var runHandlerPoolOptions: Tensorboard_RunOptions.Experimental.RunHandlerPoolOptions {
      get {return _runHandlerPoolOptions ?? Tensorboard_RunOptions.Experimental.RunHandlerPoolOptions()}
      set {_runHandlerPoolOptions = newValue}
    }
    /// Returns true if `runHandlerPoolOptions` has been explicitly set.
    var hasRunHandlerPoolOptions: Bool {return self._runHandlerPoolOptions != nil}
    /// Clears the value of `runHandlerPoolOptions`. Subsequent reads from it will return its default value.
    mutating func clearRunHandlerPoolOptions() {self._runHandlerPoolOptions = nil}

    var unknownFields = SwiftProtobuf.UnknownStorage()

    /// Options for run handler thread pool.
    struct RunHandlerPoolOptions: Sendable {
      // SwiftProtobuf.Message conformance is added in an extension below. See the
      // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
      // methods supported on all messages.

      /// Priority of the request. The run handler thread pool will schedule ops
      /// based on the priority number. The larger number means higher priority.
      var priority: Int64 = 0

      var unknownFields = SwiftProtobuf.UnknownStorage()

      init() {}
    }

    init() {}

    fileprivate var _runHandlerPoolOptions: Tensorboard_RunOptions.Experimental.RunHandlerPoolOptions? = nil
  }

  init() {}

  fileprivate var _debugOptions: Tensorboard_DebugOptions? = nil
  fileprivate var _experimental: Tensorboard_RunOptions.Experimental? = nil
}

/// Metadata output (i.e., non-Tensor) for a single Run() call.
struct Tensorboard_RunMetadata: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Statistics traced for this step. Populated if tracing is turned on via the
  /// "RunOptions" proto.
  /// EXPERIMENTAL: The format and set of events may change in future versions.
  var stepStats: Tensorboard_StepStats {
    get {return _stepStats ?? Tensorboard_StepStats()}
    set {_stepStats = newValue}
  }
  /// Returns true if `stepStats` has been explicitly set.
  var hasStepStats: Bool {return self._stepStats != nil}
  /// Clears the value of `stepStats`. Subsequent reads from it will return its default value.
  mutating func clearStepStats() {self._stepStats = nil}

  /// The cost graph for the computation defined by the run call.
  var costGraph: Tensorboard_CostGraphDef {
    get {return _costGraph ?? Tensorboard_CostGraphDef()}
    set {_costGraph = newValue}
  }
  /// Returns true if `costGraph` has been explicitly set.
  var hasCostGraph: Bool {return self._costGraph != nil}
  /// Clears the value of `costGraph`. Subsequent reads from it will return its default value.
  mutating func clearCostGraph() {self._costGraph = nil}

  /// Graphs of the partitions executed by executors.
  var partitionGraphs: [Tensorboard_GraphDef] = []

  /// This is only populated for graphs that are run as functions in TensorFlow
  /// V2. There will be an entry below for each function that is traced.
  /// The main use cases of the post_optimization_graph and the partition_graphs
  /// is to give the caller insight into the graphs that were actually run by the
  /// runtime. Additional information (such as those in step_stats) will match
  /// these graphs.
  /// We also include the pre_optimization_graph since it is usually easier to
  /// read, and is helpful in situations where the caller wants to get a high
  /// level idea of what the built graph looks like (since the various graph
  /// optimization passes might change the structure of the graph significantly).
  var functionGraphs: [Tensorboard_RunMetadata.FunctionGraphs] = []

  var unknownFields = SwiftProtobuf.UnknownStorage()

  struct FunctionGraphs: @unchecked Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    /// TODO(nareshmodi): Include some sort of function/cache-key identifier?
    var partitionGraphs: [Tensorboard_GraphDef] {
      get {return _storage._partitionGraphs}
      set {_uniqueStorage()._partitionGraphs = newValue}
    }

    var preOptimizationGraph: Tensorboard_GraphDef {
      get {return _storage._preOptimizationGraph ?? Tensorboard_GraphDef()}
      set {_uniqueStorage()._preOptimizationGraph = newValue}
    }
    /// Returns true if `preOptimizationGraph` has been explicitly set.
    var hasPreOptimizationGraph: Bool {return _storage._preOptimizationGraph != nil}
    /// Clears the value of `preOptimizationGraph`. Subsequent reads from it will return its default value.
    mutating func clearPreOptimizationGraph() {_uniqueStorage()._preOptimizationGraph = nil}

    var postOptimizationGraph: Tensorboard_GraphDef {
      get {return _storage._postOptimizationGraph ?? Tensorboard_GraphDef()}
      set {_uniqueStorage()._postOptimizationGraph = newValue}
    }
    /// Returns true if `postOptimizationGraph` has been explicitly set.
    var hasPostOptimizationGraph: Bool {return _storage._postOptimizationGraph != nil}
    /// Clears the value of `postOptimizationGraph`. Subsequent reads from it will return its default value.
    mutating func clearPostOptimizationGraph() {_uniqueStorage()._postOptimizationGraph = nil}

    var unknownFields = SwiftProtobuf.UnknownStorage()

    init() {}

    fileprivate var _storage = _StorageClass.defaultInstance
  }

  init() {}

  fileprivate var _stepStats: Tensorboard_StepStats? = nil
  fileprivate var _costGraph: Tensorboard_CostGraphDef? = nil
}

/// Defines a connection between two tensors in a `GraphDef`.
struct Tensorboard_TensorConnection: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// A tensor name. The value of this tensor will be substituted for
  /// the tensor named in `to_tensor`.
  var fromTensor: String = String()

  /// A tensor name. The value of this tensor will be bound to the
  /// value of the tensor named in `from_tensor`.
  var toTensor: String = String()

  var unknownFields = SwiftProtobuf.UnknownStorage()

  init() {}
}

/// Defines a subgraph in another `GraphDef` as a set of feed points and nodes
/// to be fetched or executed.
///
/// Compare with the arguments to `Session::Run()`.
struct Tensorboard_CallableOptions: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// Tensors to be fed in the callable. Each feed is the name of a tensor.
  var feed: [String] {
    get {return _storage._feed}
    set {_uniqueStorage()._feed = newValue}
  }

  /// Fetches. A list of tensor names. The caller of the callable expects a
  /// tensor to be returned for each fetch[i] (see RunStepResponse.tensor). The
  /// order of specified fetches does not change the execution order.
  var fetch: [String] {
    get {return _storage._fetch}
    set {_uniqueStorage()._fetch = newValue}
  }

  /// Target Nodes. A list of node names. The named nodes will be run by the
  /// callable but their outputs will not be returned.
  var target: [String] {
    get {return _storage._target}
    set {_uniqueStorage()._target = newValue}
  }

  /// Options that will be applied to each run.
  var runOptions: Tensorboard_RunOptions {
    get {return _storage._runOptions ?? Tensorboard_RunOptions()}
    set {_uniqueStorage()._runOptions = newValue}
  }
  /// Returns true if `runOptions` has been explicitly set.
  var hasRunOptions: Bool {return _storage._runOptions != nil}
  /// Clears the value of `runOptions`. Subsequent reads from it will return its default value.
  mutating func clearRunOptions() {_uniqueStorage()._runOptions = nil}

  /// Tensors to be connected in the callable. Each TensorConnection denotes
  /// a pair of tensors in the graph, between which an edge will be created
  /// in the callable.
  var tensorConnection: [Tensorboard_TensorConnection] {
    get {return _storage._tensorConnection}
    set {_uniqueStorage()._tensorConnection = newValue}
  }

  /// The Tensor objects fed in the callable and fetched from the callable
  /// are expected to be backed by host (CPU) memory by default.
  ///
  /// The options below allow changing that - feeding tensors backed by
  /// device memory, or returning tensors that are backed by device memory.
  ///
  /// The maps below map the name of a feed/fetch tensor (which appears in
  /// 'feed' or 'fetch' fields above), to the fully qualified name of the device
  /// owning the memory backing the contents of the tensor.
  ///
  /// For example, creating a callable with the following options:
  ///
  /// CallableOptions {
  ///   feed: "a:0"
  ///   feed: "b:0"
  ///
  ///   fetch: "x:0"
  ///   fetch: "y:0"
  ///
  ///   feed_devices: {
  ///     "a:0": "/job:localhost/replica:0/task:0/device:GPU:0"
  ///   }
  ///
  ///   fetch_devices: {
  ///     "y:0": "/job:localhost/replica:0/task:0/device:GPU:0"
  ///  }
  /// }
  ///
  /// means that the Callable expects:
  /// - The first argument ("a:0") is a Tensor backed by GPU memory.
  /// - The second argument ("b:0") is a Tensor backed by host memory.
  /// and of its return values:
  /// - The first output ("x:0") will be backed by host memory.
  /// - The second output ("y:0") will be backed by GPU memory.
  ///
  /// FEEDS:
  /// It is the responsibility of the caller to ensure that the memory of the fed
  /// tensors will be correctly initialized and synchronized before it is
  /// accessed by operations executed during the call to Session::RunCallable().
  ///
  /// This is typically ensured by using the TensorFlow memory allocators
  /// (Device::GetAllocator()) to create the Tensor to be fed.
  ///
  /// Alternatively, for CUDA-enabled GPU devices, this typically means that the
  /// operation that produced the contents of the tensor has completed, i.e., the
  /// CUDA stream has been synchronized (e.g., via cuCtxSynchronize() or
  /// cuStreamSynchronize()).
  var feedDevices: Dictionary<String,String> {
    get {return _storage._feedDevices}
    set {_uniqueStorage()._feedDevices = newValue}
  }

  var fetchDevices: Dictionary<String,String> {
    get {return _storage._fetchDevices}
    set {_uniqueStorage()._fetchDevices = newValue}
  }

  /// By default, RunCallable() will synchronize the GPU stream before returning
  /// fetched tensors on a GPU device, to ensure that the values in those tensors
  /// have been produced. This simplifies interacting with the tensors, but
  /// potentially incurs a performance hit.
  ///
  /// If this options is set to true, the caller is responsible for ensuring
  /// that the values in the fetched tensors have been produced before they are
  /// used. The caller can do this by invoking `Device::Sync()` on the underlying
  /// device(s), or by feeding the tensors back to the same Session using
  /// `feed_devices` with the same corresponding device name.
  var fetchSkipSync: Bool {
    get {return _storage._fetchSkipSync}
    set {_uniqueStorage()._fetchSkipSync = newValue}
  }

  var unknownFields = SwiftProtobuf.UnknownStorage()

  init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "tensorboard"

extension Tensorboard_GPUOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".GPUOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "per_process_gpu_memory_fraction"),
    4: .standard(proto: "allow_growth"),
    2: .standard(proto: "allocator_type"),
    3: .standard(proto: "deferred_deletion_bytes"),
    5: .standard(proto: "visible_device_list"),
    6: .standard(proto: "polling_active_delay_usecs"),
    7: .standard(proto: "polling_inactive_delay_msecs"),
    8: .standard(proto: "force_gpu_compatible"),
    9: .same(proto: "experimental"),
  ]

  fileprivate class _StorageClass {
    var _perProcessGpuMemoryFraction: Double = 0
    var _allowGrowth: Bool = false
    var _allocatorType: String = String()
    var _deferredDeletionBytes: Int64 = 0
    var _visibleDeviceList: String = String()
    var _pollingActiveDelayUsecs: Int32 = 0
    var _pollingInactiveDelayMsecs: Int32 = 0
    var _forceGpuCompatible: Bool = false
    var _experimental: Tensorboard_GPUOptions.Experimental? = nil

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _perProcessGpuMemoryFraction = source._perProcessGpuMemoryFraction
      _allowGrowth = source._allowGrowth
      _allocatorType = source._allocatorType
      _deferredDeletionBytes = source._deferredDeletionBytes
      _visibleDeviceList = source._visibleDeviceList
      _pollingActiveDelayUsecs = source._pollingActiveDelayUsecs
      _pollingInactiveDelayMsecs = source._pollingInactiveDelayMsecs
      _forceGpuCompatible = source._forceGpuCompatible
      _experimental = source._experimental
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularDoubleField(value: &_storage._perProcessGpuMemoryFraction) }()
        case 2: try { try decoder.decodeSingularStringField(value: &_storage._allocatorType) }()
        case 3: try { try decoder.decodeSingularInt64Field(value: &_storage._deferredDeletionBytes) }()
        case 4: try { try decoder.decodeSingularBoolField(value: &_storage._allowGrowth) }()
        case 5: try { try decoder.decodeSingularStringField(value: &_storage._visibleDeviceList) }()
        case 6: try { try decoder.decodeSingularInt32Field(value: &_storage._pollingActiveDelayUsecs) }()
        case 7: try { try decoder.decodeSingularInt32Field(value: &_storage._pollingInactiveDelayMsecs) }()
        case 8: try { try decoder.decodeSingularBoolField(value: &_storage._forceGpuCompatible) }()
        case 9: try { try decoder.decodeSingularMessageField(value: &_storage._experimental) }()
        default: break
        }
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if _storage._perProcessGpuMemoryFraction.bitPattern != 0 {
        try visitor.visitSingularDoubleField(value: _storage._perProcessGpuMemoryFraction, fieldNumber: 1)
      }
      if !_storage._allocatorType.isEmpty {
        try visitor.visitSingularStringField(value: _storage._allocatorType, fieldNumber: 2)
      }
      if _storage._deferredDeletionBytes != 0 {
        try visitor.visitSingularInt64Field(value: _storage._deferredDeletionBytes, fieldNumber: 3)
      }
      if _storage._allowGrowth != false {
        try visitor.visitSingularBoolField(value: _storage._allowGrowth, fieldNumber: 4)
      }
      if !_storage._visibleDeviceList.isEmpty {
        try visitor.visitSingularStringField(value: _storage._visibleDeviceList, fieldNumber: 5)
      }
      if _storage._pollingActiveDelayUsecs != 0 {
        try visitor.visitSingularInt32Field(value: _storage._pollingActiveDelayUsecs, fieldNumber: 6)
      }
      if _storage._pollingInactiveDelayMsecs != 0 {
        try visitor.visitSingularInt32Field(value: _storage._pollingInactiveDelayMsecs, fieldNumber: 7)
      }
      if _storage._forceGpuCompatible != false {
        try visitor.visitSingularBoolField(value: _storage._forceGpuCompatible, fieldNumber: 8)
      }
      try { if let v = _storage._experimental {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 9)
      } }()
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_GPUOptions, rhs: Tensorboard_GPUOptions) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._perProcessGpuMemoryFraction != rhs_storage._perProcessGpuMemoryFraction {return false}
        if _storage._allowGrowth != rhs_storage._allowGrowth {return false}
        if _storage._allocatorType != rhs_storage._allocatorType {return false}
        if _storage._deferredDeletionBytes != rhs_storage._deferredDeletionBytes {return false}
        if _storage._visibleDeviceList != rhs_storage._visibleDeviceList {return false}
        if _storage._pollingActiveDelayUsecs != rhs_storage._pollingActiveDelayUsecs {return false}
        if _storage._pollingInactiveDelayMsecs != rhs_storage._pollingInactiveDelayMsecs {return false}
        if _storage._forceGpuCompatible != rhs_storage._forceGpuCompatible {return false}
        if _storage._experimental != rhs_storage._experimental {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_GPUOptions.Experimental: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = Tensorboard_GPUOptions.protoMessageName + ".Experimental"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "virtual_devices"),
    2: .standard(proto: "use_unified_memory"),
    3: .standard(proto: "num_dev_to_dev_copy_streams"),
    4: .standard(proto: "collective_ring_order"),
    5: .standard(proto: "timestamped_allocator"),
    7: .standard(proto: "kernel_tracker_max_interval"),
    8: .standard(proto: "kernel_tracker_max_bytes"),
    9: .standard(proto: "kernel_tracker_max_pending"),
    10: .standard(proto: "internal_fragmentation_fraction"),
    11: .standard(proto: "use_cuda_malloc_async"),
    12: .standard(proto: "disallow_retry_on_allocation_failure"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedMessageField(value: &self.virtualDevices) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.useUnifiedMemory) }()
      case 3: try { try decoder.decodeSingularInt32Field(value: &self.numDevToDevCopyStreams) }()
      case 4: try { try decoder.decodeSingularStringField(value: &self.collectiveRingOrder) }()
      case 5: try { try decoder.decodeSingularBoolField(value: &self.timestampedAllocator) }()
      case 7: try { try decoder.decodeSingularInt32Field(value: &self.kernelTrackerMaxInterval) }()
      case 8: try { try decoder.decodeSingularInt32Field(value: &self.kernelTrackerMaxBytes) }()
      case 9: try { try decoder.decodeSingularInt32Field(value: &self.kernelTrackerMaxPending) }()
      case 10: try { try decoder.decodeSingularDoubleField(value: &self.internalFragmentationFraction) }()
      case 11: try { try decoder.decodeSingularBoolField(value: &self.useCudaMallocAsync) }()
      case 12: try { try decoder.decodeSingularBoolField(value: &self.disallowRetryOnAllocationFailure) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.virtualDevices.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.virtualDevices, fieldNumber: 1)
    }
    if self.useUnifiedMemory != false {
      try visitor.visitSingularBoolField(value: self.useUnifiedMemory, fieldNumber: 2)
    }
    if self.numDevToDevCopyStreams != 0 {
      try visitor.visitSingularInt32Field(value: self.numDevToDevCopyStreams, fieldNumber: 3)
    }
    if !self.collectiveRingOrder.isEmpty {
      try visitor.visitSingularStringField(value: self.collectiveRingOrder, fieldNumber: 4)
    }
    if self.timestampedAllocator != false {
      try visitor.visitSingularBoolField(value: self.timestampedAllocator, fieldNumber: 5)
    }
    if self.kernelTrackerMaxInterval != 0 {
      try visitor.visitSingularInt32Field(value: self.kernelTrackerMaxInterval, fieldNumber: 7)
    }
    if self.kernelTrackerMaxBytes != 0 {
      try visitor.visitSingularInt32Field(value: self.kernelTrackerMaxBytes, fieldNumber: 8)
    }
    if self.kernelTrackerMaxPending != 0 {
      try visitor.visitSingularInt32Field(value: self.kernelTrackerMaxPending, fieldNumber: 9)
    }
    if self.internalFragmentationFraction.bitPattern != 0 {
      try visitor.visitSingularDoubleField(value: self.internalFragmentationFraction, fieldNumber: 10)
    }
    if self.useCudaMallocAsync != false {
      try visitor.visitSingularBoolField(value: self.useCudaMallocAsync, fieldNumber: 11)
    }
    if self.disallowRetryOnAllocationFailure != false {
      try visitor.visitSingularBoolField(value: self.disallowRetryOnAllocationFailure, fieldNumber: 12)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_GPUOptions.Experimental, rhs: Tensorboard_GPUOptions.Experimental) -> Bool {
    if lhs.virtualDevices != rhs.virtualDevices {return false}
    if lhs.useUnifiedMemory != rhs.useUnifiedMemory {return false}
    if lhs.numDevToDevCopyStreams != rhs.numDevToDevCopyStreams {return false}
    if lhs.collectiveRingOrder != rhs.collectiveRingOrder {return false}
    if lhs.timestampedAllocator != rhs.timestampedAllocator {return false}
    if lhs.kernelTrackerMaxInterval != rhs.kernelTrackerMaxInterval {return false}
    if lhs.kernelTrackerMaxBytes != rhs.kernelTrackerMaxBytes {return false}
    if lhs.kernelTrackerMaxPending != rhs.kernelTrackerMaxPending {return false}
    if lhs.internalFragmentationFraction != rhs.internalFragmentationFraction {return false}
    if lhs.useCudaMallocAsync != rhs.useCudaMallocAsync {return false}
    if lhs.disallowRetryOnAllocationFailure != rhs.disallowRetryOnAllocationFailure {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_GPUOptions.Experimental.VirtualDevices: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = Tensorboard_GPUOptions.Experimental.protoMessageName + ".VirtualDevices"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "memory_limit_mb"),
    2: .same(proto: "priority"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedFloatField(value: &self.memoryLimitMb) }()
      case 2: try { try decoder.decodeRepeatedInt32Field(value: &self.priority) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.memoryLimitMb.isEmpty {
      try visitor.visitPackedFloatField(value: self.memoryLimitMb, fieldNumber: 1)
    }
    if !self.priority.isEmpty {
      try visitor.visitPackedInt32Field(value: self.priority, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_GPUOptions.Experimental.VirtualDevices, rhs: Tensorboard_GPUOptions.Experimental.VirtualDevices) -> Bool {
    if lhs.memoryLimitMb != rhs.memoryLimitMb {return false}
    if lhs.priority != rhs.priority {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_OptimizerOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".OptimizerOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "do_common_subexpression_elimination"),
    2: .standard(proto: "do_constant_folding"),
    6: .standard(proto: "max_folded_constant_in_bytes"),
    4: .standard(proto: "do_function_inlining"),
    3: .standard(proto: "opt_level"),
    5: .standard(proto: "global_jit_level"),
    7: .standard(proto: "cpu_global_jit"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBoolField(value: &self.doCommonSubexpressionElimination) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.doConstantFolding) }()
      case 3: try { try decoder.decodeSingularEnumField(value: &self.optLevel) }()
      case 4: try { try decoder.decodeSingularBoolField(value: &self.doFunctionInlining) }()
      case 5: try { try decoder.decodeSingularEnumField(value: &self.globalJitLevel) }()
      case 6: try { try decoder.decodeSingularInt64Field(value: &self.maxFoldedConstantInBytes) }()
      case 7: try { try decoder.decodeSingularBoolField(value: &self.cpuGlobalJit) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.doCommonSubexpressionElimination != false {
      try visitor.visitSingularBoolField(value: self.doCommonSubexpressionElimination, fieldNumber: 1)
    }
    if self.doConstantFolding != false {
      try visitor.visitSingularBoolField(value: self.doConstantFolding, fieldNumber: 2)
    }
    if self.optLevel != .l1 {
      try visitor.visitSingularEnumField(value: self.optLevel, fieldNumber: 3)
    }
    if self.doFunctionInlining != false {
      try visitor.visitSingularBoolField(value: self.doFunctionInlining, fieldNumber: 4)
    }
    if self.globalJitLevel != .default {
      try visitor.visitSingularEnumField(value: self.globalJitLevel, fieldNumber: 5)
    }
    if self.maxFoldedConstantInBytes != 0 {
      try visitor.visitSingularInt64Field(value: self.maxFoldedConstantInBytes, fieldNumber: 6)
    }
    if self.cpuGlobalJit != false {
      try visitor.visitSingularBoolField(value: self.cpuGlobalJit, fieldNumber: 7)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_OptimizerOptions, rhs: Tensorboard_OptimizerOptions) -> Bool {
    if lhs.doCommonSubexpressionElimination != rhs.doCommonSubexpressionElimination {return false}
    if lhs.doConstantFolding != rhs.doConstantFolding {return false}
    if lhs.maxFoldedConstantInBytes != rhs.maxFoldedConstantInBytes {return false}
    if lhs.doFunctionInlining != rhs.doFunctionInlining {return false}
    if lhs.optLevel != rhs.optLevel {return false}
    if lhs.globalJitLevel != rhs.globalJitLevel {return false}
    if lhs.cpuGlobalJit != rhs.cpuGlobalJit {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_OptimizerOptions.Level: SwiftProtobuf._ProtoNameProviding {
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    -1: .same(proto: "L0"),
    0: .same(proto: "L1"),
  ]
}

extension Tensorboard_OptimizerOptions.GlobalJitLevel: SwiftProtobuf._ProtoNameProviding {
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    -1: .same(proto: "OFF"),
    0: .same(proto: "DEFAULT"),
    1: .same(proto: "ON_1"),
    2: .same(proto: "ON_2"),
  ]
}

extension Tensorboard_GraphOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".GraphOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    2: .standard(proto: "enable_recv_scheduling"),
    3: .standard(proto: "optimizer_options"),
    4: .standard(proto: "build_cost_model"),
    9: .standard(proto: "build_cost_model_after"),
    5: .standard(proto: "infer_shapes"),
    6: .standard(proto: "place_pruned_graph"),
    7: .standard(proto: "enable_bfloat16_sendrecv"),
    8: .standard(proto: "timeline_step"),
    10: .standard(proto: "rewrite_options"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 2: try { try decoder.decodeSingularBoolField(value: &self.enableRecvScheduling) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._optimizerOptions) }()
      case 4: try { try decoder.decodeSingularInt64Field(value: &self.buildCostModel) }()
      case 5: try { try decoder.decodeSingularBoolField(value: &self.inferShapes) }()
      case 6: try { try decoder.decodeSingularBoolField(value: &self.placePrunedGraph) }()
      case 7: try { try decoder.decodeSingularBoolField(value: &self.enableBfloat16Sendrecv) }()
      case 8: try { try decoder.decodeSingularInt32Field(value: &self.timelineStep) }()
      case 9: try { try decoder.decodeSingularInt64Field(value: &self.buildCostModelAfter) }()
      case 10: try { try decoder.decodeSingularMessageField(value: &self._rewriteOptions) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if self.enableRecvScheduling != false {
      try visitor.visitSingularBoolField(value: self.enableRecvScheduling, fieldNumber: 2)
    }
    try { if let v = self._optimizerOptions {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    if self.buildCostModel != 0 {
      try visitor.visitSingularInt64Field(value: self.buildCostModel, fieldNumber: 4)
    }
    if self.inferShapes != false {
      try visitor.visitSingularBoolField(value: self.inferShapes, fieldNumber: 5)
    }
    if self.placePrunedGraph != false {
      try visitor.visitSingularBoolField(value: self.placePrunedGraph, fieldNumber: 6)
    }
    if self.enableBfloat16Sendrecv != false {
      try visitor.visitSingularBoolField(value: self.enableBfloat16Sendrecv, fieldNumber: 7)
    }
    if self.timelineStep != 0 {
      try visitor.visitSingularInt32Field(value: self.timelineStep, fieldNumber: 8)
    }
    if self.buildCostModelAfter != 0 {
      try visitor.visitSingularInt64Field(value: self.buildCostModelAfter, fieldNumber: 9)
    }
    try { if let v = self._rewriteOptions {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_GraphOptions, rhs: Tensorboard_GraphOptions) -> Bool {
    if lhs.enableRecvScheduling != rhs.enableRecvScheduling {return false}
    if lhs._optimizerOptions != rhs._optimizerOptions {return false}
    if lhs.buildCostModel != rhs.buildCostModel {return false}
    if lhs.buildCostModelAfter != rhs.buildCostModelAfter {return false}
    if lhs.inferShapes != rhs.inferShapes {return false}
    if lhs.placePrunedGraph != rhs.placePrunedGraph {return false}
    if lhs.enableBfloat16Sendrecv != rhs.enableBfloat16Sendrecv {return false}
    if lhs.timelineStep != rhs.timelineStep {return false}
    if lhs._rewriteOptions != rhs._rewriteOptions {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_ThreadPoolOptionProto: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".ThreadPoolOptionProto"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "num_threads"),
    2: .standard(proto: "global_name"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt32Field(value: &self.numThreads) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.globalName) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.numThreads != 0 {
      try visitor.visitSingularInt32Field(value: self.numThreads, fieldNumber: 1)
    }
    if !self.globalName.isEmpty {
      try visitor.visitSingularStringField(value: self.globalName, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_ThreadPoolOptionProto, rhs: Tensorboard_ThreadPoolOptionProto) -> Bool {
    if lhs.numThreads != rhs.numThreads {return false}
    if lhs.globalName != rhs.globalName {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_RPCOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".RPCOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "use_rpc_for_inprocess_master"),
    2: .standard(proto: "compression_algorithm"),
    3: .standard(proto: "compression_level"),
    4: .standard(proto: "cache_rpc_response"),
    5: .standard(proto: "disable_session_connection_sharing"),
    6: .standard(proto: "num_channels_per_target"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBoolField(value: &self.useRpcForInprocessMaster) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.compressionAlgorithm) }()
      case 3: try { try decoder.decodeSingularInt32Field(value: &self.compressionLevel) }()
      case 4: try { try decoder.decodeSingularBoolField(value: &self.cacheRpcResponse) }()
      case 5: try { try decoder.decodeSingularBoolField(value: &self.disableSessionConnectionSharing) }()
      case 6: try { try decoder.decodeSingularInt32Field(value: &self.numChannelsPerTarget) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.useRpcForInprocessMaster != false {
      try visitor.visitSingularBoolField(value: self.useRpcForInprocessMaster, fieldNumber: 1)
    }
    if !self.compressionAlgorithm.isEmpty {
      try visitor.visitSingularStringField(value: self.compressionAlgorithm, fieldNumber: 2)
    }
    if self.compressionLevel != 0 {
      try visitor.visitSingularInt32Field(value: self.compressionLevel, fieldNumber: 3)
    }
    if self.cacheRpcResponse != false {
      try visitor.visitSingularBoolField(value: self.cacheRpcResponse, fieldNumber: 4)
    }
    if self.disableSessionConnectionSharing != false {
      try visitor.visitSingularBoolField(value: self.disableSessionConnectionSharing, fieldNumber: 5)
    }
    if self.numChannelsPerTarget != 0 {
      try visitor.visitSingularInt32Field(value: self.numChannelsPerTarget, fieldNumber: 6)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_RPCOptions, rhs: Tensorboard_RPCOptions) -> Bool {
    if lhs.useRpcForInprocessMaster != rhs.useRpcForInprocessMaster {return false}
    if lhs.compressionAlgorithm != rhs.compressionAlgorithm {return false}
    if lhs.compressionLevel != rhs.compressionLevel {return false}
    if lhs.cacheRpcResponse != rhs.cacheRpcResponse {return false}
    if lhs.disableSessionConnectionSharing != rhs.disableSessionConnectionSharing {return false}
    if lhs.numChannelsPerTarget != rhs.numChannelsPerTarget {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_SessionMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".SessionMetadata"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .same(proto: "version"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeSingularInt64Field(value: &self.version) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if self.version != 0 {
      try visitor.visitSingularInt64Field(value: self.version, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_SessionMetadata, rhs: Tensorboard_SessionMetadata) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.version != rhs.version {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_ConfigProto: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".ConfigProto"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "device_count"),
    2: .standard(proto: "intra_op_parallelism_threads"),
    5: .standard(proto: "inter_op_parallelism_threads"),
    9: .standard(proto: "use_per_session_threads"),
    12: .standard(proto: "session_inter_op_thread_pool"),
    3: .standard(proto: "placement_period"),
    4: .standard(proto: "device_filters"),
    6: .standard(proto: "gpu_options"),
    7: .standard(proto: "allow_soft_placement"),
    8: .standard(proto: "log_device_placement"),
    10: .standard(proto: "graph_options"),
    11: .standard(proto: "operation_timeout_in_ms"),
    13: .standard(proto: "rpc_options"),
    14: .standard(proto: "cluster_def"),
    15: .standard(proto: "isolate_session_state"),
    17: .standard(proto: "share_cluster_devices_in_session"),
    16: .same(proto: "experimental"),
  ]

  fileprivate class _StorageClass {
    var _deviceCount: Dictionary<String,Int32> = [:]
    var _intraOpParallelismThreads: Int32 = 0
    var _interOpParallelismThreads: Int32 = 0
    var _usePerSessionThreads: Bool = false
    var _sessionInterOpThreadPool: [Tensorboard_ThreadPoolOptionProto] = []
    var _placementPeriod: Int32 = 0
    var _deviceFilters: [String] = []
    var _gpuOptions: Tensorboard_GPUOptions? = nil
    var _allowSoftPlacement: Bool = false
    var _logDevicePlacement: Bool = false
    var _graphOptions: Tensorboard_GraphOptions? = nil
    var _operationTimeoutInMs: Int64 = 0
    var _rpcOptions: Tensorboard_RPCOptions? = nil
    var _clusterDef: Tensorboard_ClusterDef? = nil
    var _isolateSessionState: Bool = false
    var _shareClusterDevicesInSession: Bool = false
    var _experimental: Tensorboard_ConfigProto.Experimental? = nil

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _deviceCount = source._deviceCount
      _intraOpParallelismThreads = source._intraOpParallelismThreads
      _interOpParallelismThreads = source._interOpParallelismThreads
      _usePerSessionThreads = source._usePerSessionThreads
      _sessionInterOpThreadPool = source._sessionInterOpThreadPool
      _placementPeriod = source._placementPeriod
      _deviceFilters = source._deviceFilters
      _gpuOptions = source._gpuOptions
      _allowSoftPlacement = source._allowSoftPlacement
      _logDevicePlacement = source._logDevicePlacement
      _graphOptions = source._graphOptions
      _operationTimeoutInMs = source._operationTimeoutInMs
      _rpcOptions = source._rpcOptions
      _clusterDef = source._clusterDef
      _isolateSessionState = source._isolateSessionState
      _shareClusterDevicesInSession = source._shareClusterDevicesInSession
      _experimental = source._experimental
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufInt32>.self, value: &_storage._deviceCount) }()
        case 2: try { try decoder.decodeSingularInt32Field(value: &_storage._intraOpParallelismThreads) }()
        case 3: try { try decoder.decodeSingularInt32Field(value: &_storage._placementPeriod) }()
        case 4: try { try decoder.decodeRepeatedStringField(value: &_storage._deviceFilters) }()
        case 5: try { try decoder.decodeSingularInt32Field(value: &_storage._interOpParallelismThreads) }()
        case 6: try { try decoder.decodeSingularMessageField(value: &_storage._gpuOptions) }()
        case 7: try { try decoder.decodeSingularBoolField(value: &_storage._allowSoftPlacement) }()
        case 8: try { try decoder.decodeSingularBoolField(value: &_storage._logDevicePlacement) }()
        case 9: try { try decoder.decodeSingularBoolField(value: &_storage._usePerSessionThreads) }()
        case 10: try { try decoder.decodeSingularMessageField(value: &_storage._graphOptions) }()
        case 11: try { try decoder.decodeSingularInt64Field(value: &_storage._operationTimeoutInMs) }()
        case 12: try { try decoder.decodeRepeatedMessageField(value: &_storage._sessionInterOpThreadPool) }()
        case 13: try { try decoder.decodeSingularMessageField(value: &_storage._rpcOptions) }()
        case 14: try { try decoder.decodeSingularMessageField(value: &_storage._clusterDef) }()
        case 15: try { try decoder.decodeSingularBoolField(value: &_storage._isolateSessionState) }()
        case 16: try { try decoder.decodeSingularMessageField(value: &_storage._experimental) }()
        case 17: try { try decoder.decodeSingularBoolField(value: &_storage._shareClusterDevicesInSession) }()
        default: break
        }
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._deviceCount.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufInt32>.self, value: _storage._deviceCount, fieldNumber: 1)
      }
      if _storage._intraOpParallelismThreads != 0 {
        try visitor.visitSingularInt32Field(value: _storage._intraOpParallelismThreads, fieldNumber: 2)
      }
      if _storage._placementPeriod != 0 {
        try visitor.visitSingularInt32Field(value: _storage._placementPeriod, fieldNumber: 3)
      }
      if !_storage._deviceFilters.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._deviceFilters, fieldNumber: 4)
      }
      if _storage._interOpParallelismThreads != 0 {
        try visitor.visitSingularInt32Field(value: _storage._interOpParallelismThreads, fieldNumber: 5)
      }
      try { if let v = _storage._gpuOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
      } }()
      if _storage._allowSoftPlacement != false {
        try visitor.visitSingularBoolField(value: _storage._allowSoftPlacement, fieldNumber: 7)
      }
      if _storage._logDevicePlacement != false {
        try visitor.visitSingularBoolField(value: _storage._logDevicePlacement, fieldNumber: 8)
      }
      if _storage._usePerSessionThreads != false {
        try visitor.visitSingularBoolField(value: _storage._usePerSessionThreads, fieldNumber: 9)
      }
      try { if let v = _storage._graphOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 10)
      } }()
      if _storage._operationTimeoutInMs != 0 {
        try visitor.visitSingularInt64Field(value: _storage._operationTimeoutInMs, fieldNumber: 11)
      }
      if !_storage._sessionInterOpThreadPool.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._sessionInterOpThreadPool, fieldNumber: 12)
      }
      try { if let v = _storage._rpcOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 13)
      } }()
      try { if let v = _storage._clusterDef {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 14)
      } }()
      if _storage._isolateSessionState != false {
        try visitor.visitSingularBoolField(value: _storage._isolateSessionState, fieldNumber: 15)
      }
      try { if let v = _storage._experimental {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 16)
      } }()
      if _storage._shareClusterDevicesInSession != false {
        try visitor.visitSingularBoolField(value: _storage._shareClusterDevicesInSession, fieldNumber: 17)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_ConfigProto, rhs: Tensorboard_ConfigProto) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._deviceCount != rhs_storage._deviceCount {return false}
        if _storage._intraOpParallelismThreads != rhs_storage._intraOpParallelismThreads {return false}
        if _storage._interOpParallelismThreads != rhs_storage._interOpParallelismThreads {return false}
        if _storage._usePerSessionThreads != rhs_storage._usePerSessionThreads {return false}
        if _storage._sessionInterOpThreadPool != rhs_storage._sessionInterOpThreadPool {return false}
        if _storage._placementPeriod != rhs_storage._placementPeriod {return false}
        if _storage._deviceFilters != rhs_storage._deviceFilters {return false}
        if _storage._gpuOptions != rhs_storage._gpuOptions {return false}
        if _storage._allowSoftPlacement != rhs_storage._allowSoftPlacement {return false}
        if _storage._logDevicePlacement != rhs_storage._logDevicePlacement {return false}
        if _storage._graphOptions != rhs_storage._graphOptions {return false}
        if _storage._operationTimeoutInMs != rhs_storage._operationTimeoutInMs {return false}
        if _storage._rpcOptions != rhs_storage._rpcOptions {return false}
        if _storage._clusterDef != rhs_storage._clusterDef {return false}
        if _storage._isolateSessionState != rhs_storage._isolateSessionState {return false}
        if _storage._shareClusterDevicesInSession != rhs_storage._shareClusterDevicesInSession {return false}
        if _storage._experimental != rhs_storage._experimental {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_ConfigProto.Experimental: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = Tensorboard_ConfigProto.protoMessageName + ".Experimental"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "collective_group_leader"),
    3: .standard(proto: "executor_type"),
    4: .standard(proto: "recv_buf_max_chunk"),
    5: .standard(proto: "use_numa_affinity"),
    6: .standard(proto: "collective_deterministic_sequential_execution"),
    7: .standard(proto: "collective_nccl"),
    8: .standard(proto: "share_session_state_in_clusterspec_propagation"),
    9: .standard(proto: "disable_thread_spinning"),
    10: .standard(proto: "share_cluster_devices_in_session"),
    11: .standard(proto: "session_metadata"),
    12: .standard(proto: "optimize_for_static_graph"),
    13: .standard(proto: "enable_mlir_bridge"),
    17: .standard(proto: "mlir_bridge_rollout"),
    16: .standard(proto: "enable_mlir_graph_optimization"),
    14: .standard(proto: "disable_output_partition_graphs"),
    15: .standard(proto: "xla_fusion_autotuner_thresh"),
    18: .standard(proto: "use_tfrt"),
    21: .standard(proto: "disable_functional_ops_lowering"),
    22: .standard(proto: "xla_prefer_single_graph_cluster"),
    23: .standard(proto: "coordination_config"),
  ]

  fileprivate class _StorageClass {
    var _collectiveGroupLeader: String = String()
    var _executorType: String = String()
    var _recvBufMaxChunk: Int32 = 0
    var _useNumaAffinity: Bool = false
    var _collectiveDeterministicSequentialExecution: Bool = false
    var _collectiveNccl: Bool = false
    var _shareSessionStateInClusterspecPropagation: Bool = false
    var _disableThreadSpinning: Bool = false
    var _shareClusterDevicesInSession: Bool = false
    var _sessionMetadata: Tensorboard_SessionMetadata? = nil
    var _optimizeForStaticGraph: Bool = false
    var _enableMlirBridge: Bool = false
    var _mlirBridgeRollout: Tensorboard_ConfigProto.Experimental.MlirBridgeRollout = .unspecified
    var _enableMlirGraphOptimization: Bool = false
    var _disableOutputPartitionGraphs: Bool = false
    var _xlaFusionAutotunerThresh: Int64 = 0
    var _useTfrt: Bool = false
    var _disableFunctionalOpsLowering: Bool = false
    var _xlaPreferSingleGraphCluster: Bool = false
    var _coordinationConfig: Tensorboard_CoordinationServiceConfig? = nil

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _collectiveGroupLeader = source._collectiveGroupLeader
      _executorType = source._executorType
      _recvBufMaxChunk = source._recvBufMaxChunk
      _useNumaAffinity = source._useNumaAffinity
      _collectiveDeterministicSequentialExecution = source._collectiveDeterministicSequentialExecution
      _collectiveNccl = source._collectiveNccl
      _shareSessionStateInClusterspecPropagation = source._shareSessionStateInClusterspecPropagation
      _disableThreadSpinning = source._disableThreadSpinning
      _shareClusterDevicesInSession = source._shareClusterDevicesInSession
      _sessionMetadata = source._sessionMetadata
      _optimizeForStaticGraph = source._optimizeForStaticGraph
      _enableMlirBridge = source._enableMlirBridge
      _mlirBridgeRollout = source._mlirBridgeRollout
      _enableMlirGraphOptimization = source._enableMlirGraphOptimization
      _disableOutputPartitionGraphs = source._disableOutputPartitionGraphs
      _xlaFusionAutotunerThresh = source._xlaFusionAutotunerThresh
      _useTfrt = source._useTfrt
      _disableFunctionalOpsLowering = source._disableFunctionalOpsLowering
      _xlaPreferSingleGraphCluster = source._xlaPreferSingleGraphCluster
      _coordinationConfig = source._coordinationConfig
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularStringField(value: &_storage._collectiveGroupLeader) }()
        case 3: try { try decoder.decodeSingularStringField(value: &_storage._executorType) }()
        case 4: try { try decoder.decodeSingularInt32Field(value: &_storage._recvBufMaxChunk) }()
        case 5: try { try decoder.decodeSingularBoolField(value: &_storage._useNumaAffinity) }()
        case 6: try { try decoder.decodeSingularBoolField(value: &_storage._collectiveDeterministicSequentialExecution) }()
        case 7: try { try decoder.decodeSingularBoolField(value: &_storage._collectiveNccl) }()
        case 8: try { try decoder.decodeSingularBoolField(value: &_storage._shareSessionStateInClusterspecPropagation) }()
        case 9: try { try decoder.decodeSingularBoolField(value: &_storage._disableThreadSpinning) }()
        case 10: try { try decoder.decodeSingularBoolField(value: &_storage._shareClusterDevicesInSession) }()
        case 11: try { try decoder.decodeSingularMessageField(value: &_storage._sessionMetadata) }()
        case 12: try { try decoder.decodeSingularBoolField(value: &_storage._optimizeForStaticGraph) }()
        case 13: try { try decoder.decodeSingularBoolField(value: &_storage._enableMlirBridge) }()
        case 14: try { try decoder.decodeSingularBoolField(value: &_storage._disableOutputPartitionGraphs) }()
        case 15: try { try decoder.decodeSingularInt64Field(value: &_storage._xlaFusionAutotunerThresh) }()
        case 16: try { try decoder.decodeSingularBoolField(value: &_storage._enableMlirGraphOptimization) }()
        case 17: try { try decoder.decodeSingularEnumField(value: &_storage._mlirBridgeRollout) }()
        case 18: try { try decoder.decodeSingularBoolField(value: &_storage._useTfrt) }()
        case 21: try { try decoder.decodeSingularBoolField(value: &_storage._disableFunctionalOpsLowering) }()
        case 22: try { try decoder.decodeSingularBoolField(value: &_storage._xlaPreferSingleGraphCluster) }()
        case 23: try { try decoder.decodeSingularMessageField(value: &_storage._coordinationConfig) }()
        default: break
        }
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._collectiveGroupLeader.isEmpty {
        try visitor.visitSingularStringField(value: _storage._collectiveGroupLeader, fieldNumber: 1)
      }
      if !_storage._executorType.isEmpty {
        try visitor.visitSingularStringField(value: _storage._executorType, fieldNumber: 3)
      }
      if _storage._recvBufMaxChunk != 0 {
        try visitor.visitSingularInt32Field(value: _storage._recvBufMaxChunk, fieldNumber: 4)
      }
      if _storage._useNumaAffinity != false {
        try visitor.visitSingularBoolField(value: _storage._useNumaAffinity, fieldNumber: 5)
      }
      if _storage._collectiveDeterministicSequentialExecution != false {
        try visitor.visitSingularBoolField(value: _storage._collectiveDeterministicSequentialExecution, fieldNumber: 6)
      }
      if _storage._collectiveNccl != false {
        try visitor.visitSingularBoolField(value: _storage._collectiveNccl, fieldNumber: 7)
      }
      if _storage._shareSessionStateInClusterspecPropagation != false {
        try visitor.visitSingularBoolField(value: _storage._shareSessionStateInClusterspecPropagation, fieldNumber: 8)
      }
      if _storage._disableThreadSpinning != false {
        try visitor.visitSingularBoolField(value: _storage._disableThreadSpinning, fieldNumber: 9)
      }
      if _storage._shareClusterDevicesInSession != false {
        try visitor.visitSingularBoolField(value: _storage._shareClusterDevicesInSession, fieldNumber: 10)
      }
      try { if let v = _storage._sessionMetadata {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 11)
      } }()
      if _storage._optimizeForStaticGraph != false {
        try visitor.visitSingularBoolField(value: _storage._optimizeForStaticGraph, fieldNumber: 12)
      }
      if _storage._enableMlirBridge != false {
        try visitor.visitSingularBoolField(value: _storage._enableMlirBridge, fieldNumber: 13)
      }
      if _storage._disableOutputPartitionGraphs != false {
        try visitor.visitSingularBoolField(value: _storage._disableOutputPartitionGraphs, fieldNumber: 14)
      }
      if _storage._xlaFusionAutotunerThresh != 0 {
        try visitor.visitSingularInt64Field(value: _storage._xlaFusionAutotunerThresh, fieldNumber: 15)
      }
      if _storage._enableMlirGraphOptimization != false {
        try visitor.visitSingularBoolField(value: _storage._enableMlirGraphOptimization, fieldNumber: 16)
      }
      if _storage._mlirBridgeRollout != .unspecified {
        try visitor.visitSingularEnumField(value: _storage._mlirBridgeRollout, fieldNumber: 17)
      }
      if _storage._useTfrt != false {
        try visitor.visitSingularBoolField(value: _storage._useTfrt, fieldNumber: 18)
      }
      if _storage._disableFunctionalOpsLowering != false {
        try visitor.visitSingularBoolField(value: _storage._disableFunctionalOpsLowering, fieldNumber: 21)
      }
      if _storage._xlaPreferSingleGraphCluster != false {
        try visitor.visitSingularBoolField(value: _storage._xlaPreferSingleGraphCluster, fieldNumber: 22)
      }
      try { if let v = _storage._coordinationConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 23)
      } }()
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_ConfigProto.Experimental, rhs: Tensorboard_ConfigProto.Experimental) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._collectiveGroupLeader != rhs_storage._collectiveGroupLeader {return false}
        if _storage._executorType != rhs_storage._executorType {return false}
        if _storage._recvBufMaxChunk != rhs_storage._recvBufMaxChunk {return false}
        if _storage._useNumaAffinity != rhs_storage._useNumaAffinity {return false}
        if _storage._collectiveDeterministicSequentialExecution != rhs_storage._collectiveDeterministicSequentialExecution {return false}
        if _storage._collectiveNccl != rhs_storage._collectiveNccl {return false}
        if _storage._shareSessionStateInClusterspecPropagation != rhs_storage._shareSessionStateInClusterspecPropagation {return false}
        if _storage._disableThreadSpinning != rhs_storage._disableThreadSpinning {return false}
        if _storage._shareClusterDevicesInSession != rhs_storage._shareClusterDevicesInSession {return false}
        if _storage._sessionMetadata != rhs_storage._sessionMetadata {return false}
        if _storage._optimizeForStaticGraph != rhs_storage._optimizeForStaticGraph {return false}
        if _storage._enableMlirBridge != rhs_storage._enableMlirBridge {return false}
        if _storage._mlirBridgeRollout != rhs_storage._mlirBridgeRollout {return false}
        if _storage._enableMlirGraphOptimization != rhs_storage._enableMlirGraphOptimization {return false}
        if _storage._disableOutputPartitionGraphs != rhs_storage._disableOutputPartitionGraphs {return false}
        if _storage._xlaFusionAutotunerThresh != rhs_storage._xlaFusionAutotunerThresh {return false}
        if _storage._useTfrt != rhs_storage._useTfrt {return false}
        if _storage._disableFunctionalOpsLowering != rhs_storage._disableFunctionalOpsLowering {return false}
        if _storage._xlaPreferSingleGraphCluster != rhs_storage._xlaPreferSingleGraphCluster {return false}
        if _storage._coordinationConfig != rhs_storage._coordinationConfig {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_ConfigProto.Experimental.MlirBridgeRollout: SwiftProtobuf._ProtoNameProviding {
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "MLIR_BRIDGE_ROLLOUT_UNSPECIFIED"),
    1: .same(proto: "MLIR_BRIDGE_ROLLOUT_ENABLED"),
    2: .same(proto: "MLIR_BRIDGE_ROLLOUT_DISABLED"),
    3: .same(proto: "MLIR_BRIDGE_ROLLOUT_SAFE_MODE_ENABLED"),
    4: .same(proto: "MLIR_BRIDGE_ROLLOUT_SAFE_MODE_FALLBACK_ENABLED"),
  ]
}

extension Tensorboard_RunOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".RunOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "trace_level"),
    2: .standard(proto: "timeout_in_ms"),
    3: .standard(proto: "inter_op_thread_pool"),
    5: .standard(proto: "output_partition_graphs"),
    6: .standard(proto: "debug_options"),
    7: .standard(proto: "report_tensor_allocations_upon_oom"),
    8: .same(proto: "experimental"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularEnumField(value: &self.traceLevel) }()
      case 2: try { try decoder.decodeSingularInt64Field(value: &self.timeoutInMs) }()
      case 3: try { try decoder.decodeSingularInt32Field(value: &self.interOpThreadPool) }()
      case 5: try { try decoder.decodeSingularBoolField(value: &self.outputPartitionGraphs) }()
      case 6: try { try decoder.decodeSingularMessageField(value: &self._debugOptions) }()
      case 7: try { try decoder.decodeSingularBoolField(value: &self.reportTensorAllocationsUponOom) }()
      case 8: try { try decoder.decodeSingularMessageField(value: &self._experimental) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if self.traceLevel != .noTrace {
      try visitor.visitSingularEnumField(value: self.traceLevel, fieldNumber: 1)
    }
    if self.timeoutInMs != 0 {
      try visitor.visitSingularInt64Field(value: self.timeoutInMs, fieldNumber: 2)
    }
    if self.interOpThreadPool != 0 {
      try visitor.visitSingularInt32Field(value: self.interOpThreadPool, fieldNumber: 3)
    }
    if self.outputPartitionGraphs != false {
      try visitor.visitSingularBoolField(value: self.outputPartitionGraphs, fieldNumber: 5)
    }
    try { if let v = self._debugOptions {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 6)
    } }()
    if self.reportTensorAllocationsUponOom != false {
      try visitor.visitSingularBoolField(value: self.reportTensorAllocationsUponOom, fieldNumber: 7)
    }
    try { if let v = self._experimental {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 8)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_RunOptions, rhs: Tensorboard_RunOptions) -> Bool {
    if lhs.traceLevel != rhs.traceLevel {return false}
    if lhs.timeoutInMs != rhs.timeoutInMs {return false}
    if lhs.interOpThreadPool != rhs.interOpThreadPool {return false}
    if lhs.outputPartitionGraphs != rhs.outputPartitionGraphs {return false}
    if lhs._debugOptions != rhs._debugOptions {return false}
    if lhs.reportTensorAllocationsUponOom != rhs.reportTensorAllocationsUponOom {return false}
    if lhs._experimental != rhs._experimental {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_RunOptions.TraceLevel: SwiftProtobuf._ProtoNameProviding {
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "NO_TRACE"),
    1: .same(proto: "SOFTWARE_TRACE"),
    2: .same(proto: "HARDWARE_TRACE"),
    3: .same(proto: "FULL_TRACE"),
  ]
}

extension Tensorboard_RunOptions.Experimental: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = Tensorboard_RunOptions.protoMessageName + ".Experimental"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "collective_graph_key"),
    2: .standard(proto: "use_run_handler_pool"),
    3: .standard(proto: "run_handler_pool_options"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt64Field(value: &self.collectiveGraphKey) }()
      case 2: try { try decoder.decodeSingularBoolField(value: &self.useRunHandlerPool) }()
      case 3: try { try decoder.decodeSingularMessageField(value: &self._runHandlerPoolOptions) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    if self.collectiveGraphKey != 0 {
      try visitor.visitSingularInt64Field(value: self.collectiveGraphKey, fieldNumber: 1)
    }
    if self.useRunHandlerPool != false {
      try visitor.visitSingularBoolField(value: self.useRunHandlerPool, fieldNumber: 2)
    }
    try { if let v = self._runHandlerPoolOptions {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
    } }()
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_RunOptions.Experimental, rhs: Tensorboard_RunOptions.Experimental) -> Bool {
    if lhs.collectiveGraphKey != rhs.collectiveGraphKey {return false}
    if lhs.useRunHandlerPool != rhs.useRunHandlerPool {return false}
    if lhs._runHandlerPoolOptions != rhs._runHandlerPoolOptions {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_RunOptions.Experimental.RunHandlerPoolOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = Tensorboard_RunOptions.Experimental.protoMessageName + ".RunHandlerPoolOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "priority"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularInt64Field(value: &self.priority) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.priority != 0 {
      try visitor.visitSingularInt64Field(value: self.priority, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_RunOptions.Experimental.RunHandlerPoolOptions, rhs: Tensorboard_RunOptions.Experimental.RunHandlerPoolOptions) -> Bool {
    if lhs.priority != rhs.priority {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_RunMetadata: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".RunMetadata"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "step_stats"),
    2: .standard(proto: "cost_graph"),
    3: .standard(proto: "partition_graphs"),
    4: .standard(proto: "function_graphs"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularMessageField(value: &self._stepStats) }()
      case 2: try { try decoder.decodeSingularMessageField(value: &self._costGraph) }()
      case 3: try { try decoder.decodeRepeatedMessageField(value: &self.partitionGraphs) }()
      case 4: try { try decoder.decodeRepeatedMessageField(value: &self.functionGraphs) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    // The use of inline closures is to circumvent an issue where the compiler
    // allocates stack space for every if/case branch local when no optimizations
    // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
    // https://github.com/apple/swift-protobuf/issues/1182
    try { if let v = self._stepStats {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 1)
    } }()
    try { if let v = self._costGraph {
      try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
    } }()
    if !self.partitionGraphs.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.partitionGraphs, fieldNumber: 3)
    }
    if !self.functionGraphs.isEmpty {
      try visitor.visitRepeatedMessageField(value: self.functionGraphs, fieldNumber: 4)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_RunMetadata, rhs: Tensorboard_RunMetadata) -> Bool {
    if lhs._stepStats != rhs._stepStats {return false}
    if lhs._costGraph != rhs._costGraph {return false}
    if lhs.partitionGraphs != rhs.partitionGraphs {return false}
    if lhs.functionGraphs != rhs.functionGraphs {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_RunMetadata.FunctionGraphs: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = Tensorboard_RunMetadata.protoMessageName + ".FunctionGraphs"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "partition_graphs"),
    2: .standard(proto: "pre_optimization_graph"),
    3: .standard(proto: "post_optimization_graph"),
  ]

  fileprivate class _StorageClass {
    var _partitionGraphs: [Tensorboard_GraphDef] = []
    var _preOptimizationGraph: Tensorboard_GraphDef? = nil
    var _postOptimizationGraph: Tensorboard_GraphDef? = nil

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _partitionGraphs = source._partitionGraphs
      _preOptimizationGraph = source._preOptimizationGraph
      _postOptimizationGraph = source._postOptimizationGraph
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeRepeatedMessageField(value: &_storage._partitionGraphs) }()
        case 2: try { try decoder.decodeSingularMessageField(value: &_storage._preOptimizationGraph) }()
        case 3: try { try decoder.decodeSingularMessageField(value: &_storage._postOptimizationGraph) }()
        default: break
        }
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._partitionGraphs.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._partitionGraphs, fieldNumber: 1)
      }
      try { if let v = _storage._preOptimizationGraph {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 2)
      } }()
      try { if let v = _storage._postOptimizationGraph {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 3)
      } }()
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_RunMetadata.FunctionGraphs, rhs: Tensorboard_RunMetadata.FunctionGraphs) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._partitionGraphs != rhs_storage._partitionGraphs {return false}
        if _storage._preOptimizationGraph != rhs_storage._preOptimizationGraph {return false}
        if _storage._postOptimizationGraph != rhs_storage._postOptimizationGraph {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_TensorConnection: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".TensorConnection"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "from_tensor"),
    2: .standard(proto: "to_tensor"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.fromTensor) }()
      case 2: try { try decoder.decodeSingularStringField(value: &self.toTensor) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.fromTensor.isEmpty {
      try visitor.visitSingularStringField(value: self.fromTensor, fieldNumber: 1)
    }
    if !self.toTensor.isEmpty {
      try visitor.visitSingularStringField(value: self.toTensor, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_TensorConnection, rhs: Tensorboard_TensorConnection) -> Bool {
    if lhs.fromTensor != rhs.fromTensor {return false}
    if lhs.toTensor != rhs.toTensor {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_CallableOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".CallableOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "feed"),
    2: .same(proto: "fetch"),
    3: .same(proto: "target"),
    4: .standard(proto: "run_options"),
    5: .standard(proto: "tensor_connection"),
    6: .standard(proto: "feed_devices"),
    7: .standard(proto: "fetch_devices"),
    8: .standard(proto: "fetch_skip_sync"),
  ]

  fileprivate class _StorageClass {
    var _feed: [String] = []
    var _fetch: [String] = []
    var _target: [String] = []
    var _runOptions: Tensorboard_RunOptions? = nil
    var _tensorConnection: [Tensorboard_TensorConnection] = []
    var _feedDevices: Dictionary<String,String> = [:]
    var _fetchDevices: Dictionary<String,String> = [:]
    var _fetchSkipSync: Bool = false

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _feed = source._feed
      _fetch = source._fetch
      _target = source._target
      _runOptions = source._runOptions
      _tensorConnection = source._tensorConnection
      _feedDevices = source._feedDevices
      _fetchDevices = source._fetchDevices
      _fetchSkipSync = source._fetchSkipSync
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeRepeatedStringField(value: &_storage._feed) }()
        case 2: try { try decoder.decodeRepeatedStringField(value: &_storage._fetch) }()
        case 3: try { try decoder.decodeRepeatedStringField(value: &_storage._target) }()
        case 4: try { try decoder.decodeSingularMessageField(value: &_storage._runOptions) }()
        case 5: try { try decoder.decodeRepeatedMessageField(value: &_storage._tensorConnection) }()
        case 6: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._feedDevices) }()
        case 7: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: &_storage._fetchDevices) }()
        case 8: try { try decoder.decodeSingularBoolField(value: &_storage._fetchSkipSync) }()
        default: break
        }
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if !_storage._feed.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._feed, fieldNumber: 1)
      }
      if !_storage._fetch.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._fetch, fieldNumber: 2)
      }
      if !_storage._target.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._target, fieldNumber: 3)
      }
      try { if let v = _storage._runOptions {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 4)
      } }()
      if !_storage._tensorConnection.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._tensorConnection, fieldNumber: 5)
      }
      if !_storage._feedDevices.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._feedDevices, fieldNumber: 6)
      }
      if !_storage._fetchDevices.isEmpty {
        try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMap<SwiftProtobuf.ProtobufString,SwiftProtobuf.ProtobufString>.self, value: _storage._fetchDevices, fieldNumber: 7)
      }
      if _storage._fetchSkipSync != false {
        try visitor.visitSingularBoolField(value: _storage._fetchSkipSync, fieldNumber: 8)
      }
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_CallableOptions, rhs: Tensorboard_CallableOptions) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._feed != rhs_storage._feed {return false}
        if _storage._fetch != rhs_storage._fetch {return false}
        if _storage._target != rhs_storage._target {return false}
        if _storage._runOptions != rhs_storage._runOptions {return false}
        if _storage._tensorConnection != rhs_storage._tensorConnection {return false}
        if _storage._feedDevices != rhs_storage._feedDevices {return false}
        if _storage._fetchDevices != rhs_storage._fetchDevices {return false}
        if _storage._fetchSkipSync != rhs_storage._fetchSkipSync {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
