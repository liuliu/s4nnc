// DO NOT EDIT.
// swift-format-ignore-file
//
// Generated by the Swift generator plugin for the protocol buffer compiler.
// Source: tensorboard/compat/proto/rewriter_config.proto
//
// For information on using the generated types, please see the documentation:
//   https://github.com/apple/swift-protobuf/

import Foundation
import SwiftProtobuf

// If the compiler emits an error on this type, it is because this file
// was generated by a version of the `protoc` Swift plug-in that is
// incompatible with the version of SwiftProtobuf to which you are linking.
// Please ensure that you are building against the same version of the API
// that was used to generate this file.
fileprivate struct _GeneratedWithProtocGenSwiftVersion: SwiftProtobuf.ProtobufAPIVersionCheck {
  struct _2: SwiftProtobuf.ProtobufAPIVersion_2 {}
  typealias Version = _2
}

struct Tensorboard_AutoParallelOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  var enable: Bool = false

  var numReplicas: Int32 = 0

  var unknownFields = SwiftProtobuf.UnknownStorage()

  init() {}
}

struct Tensorboard_ScopedAllocatorOptions: Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// If present, only perform optimization for these ops.
  var enableOp: [String] = []

  var unknownFields = SwiftProtobuf.UnknownStorage()

  init() {}
}

/// Graph rewriting is experimental and subject to change, not covered by any
/// API stability guarantees.
struct Tensorboard_RewriterConfig: @unchecked Sendable {
  // SwiftProtobuf.Message conformance is added in an extension below. See the
  // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
  // methods supported on all messages.

  /// CPU Conversion settings between NHCW and NCHW.
  var cpuLayoutConversion: Tensorboard_RewriterConfig.CpuLayout {
    get {return _storage._cpuLayoutConversion}
    set {_uniqueStorage()._cpuLayoutConversion = newValue}
  }

  /// Optimize tensor layouts (default is ON)
  /// e.g. This will try to use NCHW layout on GPU which is faster.
  var layoutOptimizer: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._layoutOptimizer}
    set {_uniqueStorage()._layoutOptimizer = newValue}
  }

  /// Fold constants (default is ON)
  /// Statically infer the value of tensors when possible, and materialize the
  /// result using constants.
  var constantFolding: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._constantFolding}
    set {_uniqueStorage()._constantFolding = newValue}
  }

  /// Shape optimizations (default is ON)
  /// Simplify computations made on shapes.
  var shapeOptimization: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._shapeOptimization}
    set {_uniqueStorage()._shapeOptimization = newValue}
  }

  /// Remapping (default is ON)
  /// Remap subgraphs onto more efficient implementations.
  var remapping: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._remapping}
    set {_uniqueStorage()._remapping = newValue}
  }

  /// Common subgraph elimination (default is ON)
  /// e.g. Simplify arithmetic ops; merge ops with same value (like constants).
  var commonSubgraphElimination: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._commonSubgraphElimination}
    set {_uniqueStorage()._commonSubgraphElimination = newValue}
  }

  /// Arithmetic optimizations (default is ON)
  /// e.g. Simplify arithmetic ops; merge ops with same value (like constants).
  var arithmeticOptimization: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._arithmeticOptimization}
    set {_uniqueStorage()._arithmeticOptimization = newValue}
  }

  /// Control dependency optimizations (default is ON).
  /// Remove redundant control dependencies, which may enable other optimization.
  var dependencyOptimization: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._dependencyOptimization}
    set {_uniqueStorage()._dependencyOptimization = newValue}
  }

  /// Loop optimizations (default is ON).
  var loopOptimization: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._loopOptimization}
    set {_uniqueStorage()._loopOptimization = newValue}
  }

  /// Function optimizations (default is ON).
  var functionOptimization: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._functionOptimization}
    set {_uniqueStorage()._functionOptimization = newValue}
  }

  /// Strips debug-related nodes from the graph (off by default).
  var debugStripper: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._debugStripper}
    set {_uniqueStorage()._debugStripper = newValue}
  }

  /// If true, don't remove unnecessary ops from the graph
  var disableModelPruning: Bool {
    get {return _storage._disableModelPruning}
    set {_uniqueStorage()._disableModelPruning = newValue}
  }

  /// Try to allocate some independent Op outputs contiguously in order to
  /// merge or eliminate downstream Ops (off by default).
  var scopedAllocatorOptimization: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._scopedAllocatorOptimization}
    set {_uniqueStorage()._scopedAllocatorOptimization = newValue}
  }

  /// Force small ops onto the CPU (default is OFF).
  var pinToHostOptimization: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._pinToHostOptimization}
    set {_uniqueStorage()._pinToHostOptimization = newValue}
  }

  /// Enable the swap of kernel implementations based on the device placement
  /// (default is ON).
  var implementationSelector: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._implementationSelector}
    set {_uniqueStorage()._implementationSelector = newValue}
  }

  /// Optimize data types for CUDA (default is OFF).
  /// This will try to use float16 on GPU which is faster.
  /// Note that this can change the numerical stability of the graph and may
  /// require the use of loss scaling to maintain model convergence.
  var autoMixedPrecision: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._autoMixedPrecision}
    set {_uniqueStorage()._autoMixedPrecision = newValue}
  }

  /// Optimize data types for MKL (default is OFF).
  /// This will try to use bfloat16 on CPUs, which is faster.
  /// Note that this can change the numerical stability of the graph.
  var autoMixedPrecisionMkl: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._autoMixedPrecisionMkl}
    set {_uniqueStorage()._autoMixedPrecisionMkl = newValue}
  }

  /// Emulate a model using data type float16 on CPU (default is OFF).
  /// This will try to emulate the float16 inputs and outputs of an operator
  /// on CPU to have better correlation with float16 on GPU; however the
  /// computation in the operator is based on float32.
  /// Note that this can change the numerical stability of the graph.
  var autoMixedPrecisionCpu: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._autoMixedPrecisionCpu}
    set {_uniqueStorage()._autoMixedPrecisionCpu = newValue}
  }

  /// Disable the entire meta optimizer (off by default).
  var disableMetaOptimizer: Bool {
    get {return _storage._disableMetaOptimizer}
    set {_uniqueStorage()._disableMetaOptimizer = newValue}
  }

  /// Optimizers registered by plugin (default is ON)
  var usePluginOptimizers: Tensorboard_RewriterConfig.Toggle {
    get {return _storage._usePluginOptimizers}
    set {_uniqueStorage()._usePluginOptimizers = newValue}
  }

  /// Controls how many times we run the optimizers in meta optimizer (default
  /// is once).
  var metaOptimizerIterations: Tensorboard_RewriterConfig.NumIterationsType {
    get {return _storage._metaOptimizerIterations}
    set {_uniqueStorage()._metaOptimizerIterations = newValue}
  }

  /// The minimum number of nodes in a graph to optimizer. For smaller graphs,
  /// optimization is skipped.
  /// 0 means the system picks an appropriate number.
  /// < 0 means do not skip optimization.
  var minGraphNodes: Int32 {
    get {return _storage._minGraphNodes}
    set {_uniqueStorage()._minGraphNodes = newValue}
  }

  /// Disable optimizations that assume compressed tensors. Note that this flag
  /// is experimental and may be removed in the future.
  var experimentalDisableCompressedTensorOptimization: Bool {
    get {return _storage._experimentalDisableCompressedTensorOptimization}
    set {_uniqueStorage()._experimentalDisableCompressedTensorOptimization = newValue}
  }

  /// Disable folding quantization emulation ops such as FakeQuantWithMinMax* and
  /// QuantizeAndDequantize*. Some compilers (e.g. the TF-to-tflite converter)
  /// have to extract quantization configs (e.g. min/max range, number of bits,
  /// and per-channel) from the quantization emulation ops. Note that this flag
  /// is experimental and may be removed in the future. See b/174138564 for more
  /// details.
  var experimentalDisableFoldingQuantizationEmulation: Bool {
    get {return _storage._experimentalDisableFoldingQuantizationEmulation}
    set {_uniqueStorage()._experimentalDisableFoldingQuantizationEmulation = newValue}
  }

  /// Configures memory optimization passes through the meta-optimizer. Has no
  /// effect on manually requested memory optimization passes in the optimizers
  /// field.
  var memoryOptimization: Tensorboard_RewriterConfig.MemOptType {
    get {return _storage._memoryOptimization}
    set {_uniqueStorage()._memoryOptimization = newValue}
  }

  /// A node name scope for node names which are valid outputs of recomputations.
  /// Inputs to nodes that match this scope may be recomputed (subject either to
  /// manual annotation of those input nodes or to manual annotation and
  /// heuristics depending on memory_optimization), but the nodes themselves will
  /// not be recomputed. This matches any sub-scopes as well, meaning the scope
  /// can appear not just as a top-level scope. For example, if the value is
  /// "gradients/", the default, it will match node name "gradients/foo",
  /// "foo/gradients/bar", but not "foo_gradients/"
  var memoryOptimizerTargetNodeNameScope: String {
    get {return _storage._memoryOptimizerTargetNodeNameScope}
    set {_uniqueStorage()._memoryOptimizerTargetNodeNameScope = newValue}
  }

  /// Maximum number of milliseconds to spend optimizing a single graph before
  /// timing out. If less than or equal to 0 (default value) the optimizer will
  /// never time out.
  var metaOptimizerTimeoutMs: Int64 {
    get {return _storage._metaOptimizerTimeoutMs}
    set {_uniqueStorage()._metaOptimizerTimeoutMs = newValue}
  }

  /// Configures AutoParallel optimization passes either through the
  /// meta-optimizer or when manually specified through the optimizers field.
  var autoParallel: Tensorboard_AutoParallelOptions {
    get {return _storage._autoParallel ?? Tensorboard_AutoParallelOptions()}
    set {_uniqueStorage()._autoParallel = newValue}
  }
  /// Returns true if `autoParallel` has been explicitly set.
  var hasAutoParallel: Bool {return _storage._autoParallel != nil}
  /// Clears the value of `autoParallel`. Subsequent reads from it will return its default value.
  mutating func clearAutoParallel() {_uniqueStorage()._autoParallel = nil}

  /// If true, any optimization pass failing will cause the MetaOptimizer to
  /// stop with an error. By default - or when set to false, failing passes are
  /// skipped silently.
  var failOnOptimizerErrors: Bool {
    get {return _storage._failOnOptimizerErrors}
    set {_uniqueStorage()._failOnOptimizerErrors = newValue}
  }

  var scopedAllocatorOpts: Tensorboard_ScopedAllocatorOptions {
    get {return _storage._scopedAllocatorOpts ?? Tensorboard_ScopedAllocatorOptions()}
    set {_uniqueStorage()._scopedAllocatorOpts = newValue}
  }
  /// Returns true if `scopedAllocatorOpts` has been explicitly set.
  var hasScopedAllocatorOpts: Bool {return _storage._scopedAllocatorOpts != nil}
  /// Clears the value of `scopedAllocatorOpts`. Subsequent reads from it will return its default value.
  mutating func clearScopedAllocatorOpts() {_uniqueStorage()._scopedAllocatorOpts = nil}

  /// If non-empty, will use this as an alternative way to specify a list of
  /// optimizations to turn on and the order of the optimizations (replacing the
  /// meta-optimizer).
  ///
  /// Of the RewriterConfig options, only the AutoParallel configuration options
  /// (the auto_parallel field) apply to manually requested optimization passes
  /// ("autoparallel"). Memory optimization passes ("memory") invoked here are
  /// not configurable (in contrast to memory optimization passes through the
  /// meta-optimizer) and act only on manual op annotations.
  ///
  /// Custom optimizers (see custom_optimizers) that are not part of this
  /// schedule will be run after - in the order that they were specified.
  var optimizers: [String] {
    get {return _storage._optimizers}
    set {_uniqueStorage()._optimizers = newValue}
  }

  /// list of CustomGraphOptimizers to apply.
  var customOptimizers: [Tensorboard_RewriterConfig.CustomGraphOptimizer] {
    get {return _storage._customOptimizers}
    set {_uniqueStorage()._customOptimizers = newValue}
  }

  /// VerifierConfig specifying the verifiers to be run after every optimizer.
  var interOptimizerVerifierConfig: Tensorboard_VerifierConfig {
    get {return _storage._interOptimizerVerifierConfig ?? Tensorboard_VerifierConfig()}
    set {_uniqueStorage()._interOptimizerVerifierConfig = newValue}
  }
  /// Returns true if `interOptimizerVerifierConfig` has been explicitly set.
  var hasInterOptimizerVerifierConfig: Bool {return _storage._interOptimizerVerifierConfig != nil}
  /// Clears the value of `interOptimizerVerifierConfig`. Subsequent reads from it will return its default value.
  mutating func clearInterOptimizerVerifierConfig() {_uniqueStorage()._interOptimizerVerifierConfig = nil}

  /// VerifierConfig specifying the verifiers to be run at the end, after all
  /// optimizers have run.
  var postOptimizationVerifierConfig: Tensorboard_VerifierConfig {
    get {return _storage._postOptimizationVerifierConfig ?? Tensorboard_VerifierConfig()}
    set {_uniqueStorage()._postOptimizationVerifierConfig = newValue}
  }
  /// Returns true if `postOptimizationVerifierConfig` has been explicitly set.
  var hasPostOptimizationVerifierConfig: Bool {return _storage._postOptimizationVerifierConfig != nil}
  /// Clears the value of `postOptimizationVerifierConfig`. Subsequent reads from it will return its default value.
  mutating func clearPostOptimizationVerifierConfig() {_uniqueStorage()._postOptimizationVerifierConfig = nil}

  var unknownFields = SwiftProtobuf.UnknownStorage()

  enum Toggle: SwiftProtobuf.Enum, Swift.CaseIterable {
    typealias RawValue = Int
    case `default` // = 0
    case on // = 1
    case off // = 2

    /// Enable some aggressive optimizations that use assumptions that TF graphs
    /// may break. For example, assume the shape of a placeholder matches its
    /// actual feed.
    case aggressive // = 3

    /// Run MLIR pass if there's one implemented in TFG, do nothing otherwise.
    /// I.e., if there's no corresponding TFG pass, it's an OFF. This is supposed
    /// to be mapped with `ON` and there's no `AGGRESSIVE` in MLIR pass now.
    case experimentalMlir // = 4

    /// Run both MLIR and Grappler passes consecutively and MLIR pass will come
    /// first.
    case experimentalBoth // = 5
    case UNRECOGNIZED(Int)

    init() {
      self = .default
    }

    init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .default
      case 1: self = .on
      case 2: self = .off
      case 3: self = .aggressive
      case 4: self = .experimentalMlir
      case 5: self = .experimentalBoth
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    var rawValue: Int {
      switch self {
      case .default: return 0
      case .on: return 1
      case .off: return 2
      case .aggressive: return 3
      case .experimentalMlir: return 4
      case .experimentalBoth: return 5
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    static let allCases: [Tensorboard_RewriterConfig.Toggle] = [
      .default,
      .on,
      .off,
      .aggressive,
      .experimentalMlir,
      .experimentalBoth,
    ]

  }

  /// Enum for layout conversion between NCHW and NHWC on CPU. Default is OFF.
  enum CpuLayout: SwiftProtobuf.Enum, Swift.CaseIterable {
    typealias RawValue = Int
    case noConversionOnCpu // = 0
    case nchwToNhwc // = 1
    case nhwcToNchw // = 2
    case UNRECOGNIZED(Int)

    init() {
      self = .noConversionOnCpu
    }

    init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .noConversionOnCpu
      case 1: self = .nchwToNhwc
      case 2: self = .nhwcToNchw
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    var rawValue: Int {
      switch self {
      case .noConversionOnCpu: return 0
      case .nchwToNhwc: return 1
      case .nhwcToNchw: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    static let allCases: [Tensorboard_RewriterConfig.CpuLayout] = [
      .noConversionOnCpu,
      .nchwToNhwc,
      .nhwcToNchw,
    ]

  }

  /// Enum controlling the number of times to run optimizers. The default is to
  /// run them twice.
  enum NumIterationsType: SwiftProtobuf.Enum, Swift.CaseIterable {
    typealias RawValue = Int
    case defaultNumIters // = 0
    case one // = 1
    case two // = 2
    case UNRECOGNIZED(Int)

    init() {
      self = .defaultNumIters
    }

    init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .defaultNumIters
      case 1: self = .one
      case 2: self = .two
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    var rawValue: Int {
      switch self {
      case .defaultNumIters: return 0
      case .one: return 1
      case .two: return 2
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    static let allCases: [Tensorboard_RewriterConfig.NumIterationsType] = [
      .defaultNumIters,
      .one,
      .two,
    ]

  }

  enum MemOptType: SwiftProtobuf.Enum, Swift.CaseIterable {
    typealias RawValue = Int

    /// The default setting (SCHEDULING and SWAPPING HEURISTICS only)
    case defaultMemOpt // = 0

    /// Disabled in the meta-optimizer.
    case noMemOpt // = 1

    /// Driven by manual op-level annotations.
    case manual // = 2

    /// Swapping heuristic will move a tensor from the GPU to the CPU and move
    /// it back when needed to reduce peak memory usage.
    case swappingHeuristics // = 4

    /// Recomputation heuristics will recompute ops (such as Relu activation)
    /// during backprop instead of storing them, reducing peak memory usage.
    case recomputationHeuristics // = 5

    /// Scheduling will split big ops such as AddN and try to enforce a schedule
    /// of the new computations that decreases peak memory usage.
    case schedulingHeuristics // = 6

    /// Use any combination of swapping and recomputation heuristics.
    case heuristics // = 3
    case UNRECOGNIZED(Int)

    init() {
      self = .defaultMemOpt
    }

    init?(rawValue: Int) {
      switch rawValue {
      case 0: self = .defaultMemOpt
      case 1: self = .noMemOpt
      case 2: self = .manual
      case 3: self = .heuristics
      case 4: self = .swappingHeuristics
      case 5: self = .recomputationHeuristics
      case 6: self = .schedulingHeuristics
      default: self = .UNRECOGNIZED(rawValue)
      }
    }

    var rawValue: Int {
      switch self {
      case .defaultMemOpt: return 0
      case .noMemOpt: return 1
      case .manual: return 2
      case .heuristics: return 3
      case .swappingHeuristics: return 4
      case .recomputationHeuristics: return 5
      case .schedulingHeuristics: return 6
      case .UNRECOGNIZED(let i): return i
      }
    }

    // The compiler won't synthesize support with the UNRECOGNIZED case.
    static let allCases: [Tensorboard_RewriterConfig.MemOptType] = [
      .defaultMemOpt,
      .noMemOpt,
      .manual,
      .swappingHeuristics,
      .recomputationHeuristics,
      .schedulingHeuristics,
      .heuristics,
    ]

  }

  /// Message to describe custom graph optimizer and its parameters
  struct CustomGraphOptimizer: Sendable {
    // SwiftProtobuf.Message conformance is added in an extension below. See the
    // `Message` and `Message+*Additions` files in the SwiftProtobuf library for
    // methods supported on all messages.

    var name: String = String()

    var parameterMap: Dictionary<String,Tensorboard_AttrValue> = [:]

    var unknownFields = SwiftProtobuf.UnknownStorage()

    init() {}
  }

  init() {}

  fileprivate var _storage = _StorageClass.defaultInstance
}

// MARK: - Code below here is support for the SwiftProtobuf runtime.

fileprivate let _protobuf_package = "tensorboard"

extension Tensorboard_AutoParallelOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".AutoParallelOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "enable"),
    2: .standard(proto: "num_replicas"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularBoolField(value: &self.enable) }()
      case 2: try { try decoder.decodeSingularInt32Field(value: &self.numReplicas) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if self.enable != false {
      try visitor.visitSingularBoolField(value: self.enable, fieldNumber: 1)
    }
    if self.numReplicas != 0 {
      try visitor.visitSingularInt32Field(value: self.numReplicas, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_AutoParallelOptions, rhs: Tensorboard_AutoParallelOptions) -> Bool {
    if lhs.enable != rhs.enable {return false}
    if lhs.numReplicas != rhs.numReplicas {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_ScopedAllocatorOptions: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".ScopedAllocatorOptions"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .standard(proto: "enable_op"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeRepeatedStringField(value: &self.enableOp) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.enableOp.isEmpty {
      try visitor.visitRepeatedStringField(value: self.enableOp, fieldNumber: 1)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_ScopedAllocatorOptions, rhs: Tensorboard_ScopedAllocatorOptions) -> Bool {
    if lhs.enableOp != rhs.enableOp {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_RewriterConfig: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = _protobuf_package + ".RewriterConfig"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    50: .standard(proto: "cpu_layout_conversion"),
    1: .standard(proto: "layout_optimizer"),
    3: .standard(proto: "constant_folding"),
    13: .standard(proto: "shape_optimization"),
    14: .same(proto: "remapping"),
    24: .standard(proto: "common_subgraph_elimination"),
    7: .standard(proto: "arithmetic_optimization"),
    8: .standard(proto: "dependency_optimization"),
    9: .standard(proto: "loop_optimization"),
    10: .standard(proto: "function_optimization"),
    11: .standard(proto: "debug_stripper"),
    2: .standard(proto: "disable_model_pruning"),
    15: .standard(proto: "scoped_allocator_optimization"),
    18: .standard(proto: "pin_to_host_optimization"),
    22: .standard(proto: "implementation_selector"),
    23: .standard(proto: "auto_mixed_precision"),
    25: .standard(proto: "auto_mixed_precision_mkl"),
    29: .standard(proto: "auto_mixed_precision_cpu"),
    19: .standard(proto: "disable_meta_optimizer"),
    28: .standard(proto: "use_plugin_optimizers"),
    12: .standard(proto: "meta_optimizer_iterations"),
    17: .standard(proto: "min_graph_nodes"),
    26: .standard(proto: "experimental_disable_compressed_tensor_optimization"),
    27: .standard(proto: "experimental_disable_folding_quantization_emulation"),
    4: .standard(proto: "memory_optimization"),
    6: .standard(proto: "memory_optimizer_target_node_name_scope"),
    20: .standard(proto: "meta_optimizer_timeout_ms"),
    5: .standard(proto: "auto_parallel"),
    21: .standard(proto: "fail_on_optimizer_errors"),
    16: .standard(proto: "scoped_allocator_opts"),
    100: .same(proto: "optimizers"),
    200: .standard(proto: "custom_optimizers"),
    300: .standard(proto: "inter_optimizer_verifier_config"),
    301: .standard(proto: "post_optimization_verifier_config"),
  ]

  fileprivate class _StorageClass {
    var _cpuLayoutConversion: Tensorboard_RewriterConfig.CpuLayout = .noConversionOnCpu
    var _layoutOptimizer: Tensorboard_RewriterConfig.Toggle = .default
    var _constantFolding: Tensorboard_RewriterConfig.Toggle = .default
    var _shapeOptimization: Tensorboard_RewriterConfig.Toggle = .default
    var _remapping: Tensorboard_RewriterConfig.Toggle = .default
    var _commonSubgraphElimination: Tensorboard_RewriterConfig.Toggle = .default
    var _arithmeticOptimization: Tensorboard_RewriterConfig.Toggle = .default
    var _dependencyOptimization: Tensorboard_RewriterConfig.Toggle = .default
    var _loopOptimization: Tensorboard_RewriterConfig.Toggle = .default
    var _functionOptimization: Tensorboard_RewriterConfig.Toggle = .default
    var _debugStripper: Tensorboard_RewriterConfig.Toggle = .default
    var _disableModelPruning: Bool = false
    var _scopedAllocatorOptimization: Tensorboard_RewriterConfig.Toggle = .default
    var _pinToHostOptimization: Tensorboard_RewriterConfig.Toggle = .default
    var _implementationSelector: Tensorboard_RewriterConfig.Toggle = .default
    var _autoMixedPrecision: Tensorboard_RewriterConfig.Toggle = .default
    var _autoMixedPrecisionMkl: Tensorboard_RewriterConfig.Toggle = .default
    var _autoMixedPrecisionCpu: Tensorboard_RewriterConfig.Toggle = .default
    var _disableMetaOptimizer: Bool = false
    var _usePluginOptimizers: Tensorboard_RewriterConfig.Toggle = .default
    var _metaOptimizerIterations: Tensorboard_RewriterConfig.NumIterationsType = .defaultNumIters
    var _minGraphNodes: Int32 = 0
    var _experimentalDisableCompressedTensorOptimization: Bool = false
    var _experimentalDisableFoldingQuantizationEmulation: Bool = false
    var _memoryOptimization: Tensorboard_RewriterConfig.MemOptType = .defaultMemOpt
    var _memoryOptimizerTargetNodeNameScope: String = String()
    var _metaOptimizerTimeoutMs: Int64 = 0
    var _autoParallel: Tensorboard_AutoParallelOptions? = nil
    var _failOnOptimizerErrors: Bool = false
    var _scopedAllocatorOpts: Tensorboard_ScopedAllocatorOptions? = nil
    var _optimizers: [String] = []
    var _customOptimizers: [Tensorboard_RewriterConfig.CustomGraphOptimizer] = []
    var _interOptimizerVerifierConfig: Tensorboard_VerifierConfig? = nil
    var _postOptimizationVerifierConfig: Tensorboard_VerifierConfig? = nil

    #if swift(>=5.10)
      // This property is used as the initial default value for new instances of the type.
      // The type itself is protecting the reference to its storage via CoW semantics.
      // This will force a copy to be made of this reference when the first mutation occurs;
      // hence, it is safe to mark this as `nonisolated(unsafe)`.
      static nonisolated(unsafe) let defaultInstance = _StorageClass()
    #else
      static let defaultInstance = _StorageClass()
    #endif

    private init() {}

    init(copying source: _StorageClass) {
      _cpuLayoutConversion = source._cpuLayoutConversion
      _layoutOptimizer = source._layoutOptimizer
      _constantFolding = source._constantFolding
      _shapeOptimization = source._shapeOptimization
      _remapping = source._remapping
      _commonSubgraphElimination = source._commonSubgraphElimination
      _arithmeticOptimization = source._arithmeticOptimization
      _dependencyOptimization = source._dependencyOptimization
      _loopOptimization = source._loopOptimization
      _functionOptimization = source._functionOptimization
      _debugStripper = source._debugStripper
      _disableModelPruning = source._disableModelPruning
      _scopedAllocatorOptimization = source._scopedAllocatorOptimization
      _pinToHostOptimization = source._pinToHostOptimization
      _implementationSelector = source._implementationSelector
      _autoMixedPrecision = source._autoMixedPrecision
      _autoMixedPrecisionMkl = source._autoMixedPrecisionMkl
      _autoMixedPrecisionCpu = source._autoMixedPrecisionCpu
      _disableMetaOptimizer = source._disableMetaOptimizer
      _usePluginOptimizers = source._usePluginOptimizers
      _metaOptimizerIterations = source._metaOptimizerIterations
      _minGraphNodes = source._minGraphNodes
      _experimentalDisableCompressedTensorOptimization = source._experimentalDisableCompressedTensorOptimization
      _experimentalDisableFoldingQuantizationEmulation = source._experimentalDisableFoldingQuantizationEmulation
      _memoryOptimization = source._memoryOptimization
      _memoryOptimizerTargetNodeNameScope = source._memoryOptimizerTargetNodeNameScope
      _metaOptimizerTimeoutMs = source._metaOptimizerTimeoutMs
      _autoParallel = source._autoParallel
      _failOnOptimizerErrors = source._failOnOptimizerErrors
      _scopedAllocatorOpts = source._scopedAllocatorOpts
      _optimizers = source._optimizers
      _customOptimizers = source._customOptimizers
      _interOptimizerVerifierConfig = source._interOptimizerVerifierConfig
      _postOptimizationVerifierConfig = source._postOptimizationVerifierConfig
    }
  }

  fileprivate mutating func _uniqueStorage() -> _StorageClass {
    if !isKnownUniquelyReferenced(&_storage) {
      _storage = _StorageClass(copying: _storage)
    }
    return _storage
  }

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    _ = _uniqueStorage()
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      while let fieldNumber = try decoder.nextFieldNumber() {
        // The use of inline closures is to circumvent an issue where the compiler
        // allocates stack space for every case branch when no optimizations are
        // enabled. https://github.com/apple/swift-protobuf/issues/1034
        switch fieldNumber {
        case 1: try { try decoder.decodeSingularEnumField(value: &_storage._layoutOptimizer) }()
        case 2: try { try decoder.decodeSingularBoolField(value: &_storage._disableModelPruning) }()
        case 3: try { try decoder.decodeSingularEnumField(value: &_storage._constantFolding) }()
        case 4: try { try decoder.decodeSingularEnumField(value: &_storage._memoryOptimization) }()
        case 5: try { try decoder.decodeSingularMessageField(value: &_storage._autoParallel) }()
        case 6: try { try decoder.decodeSingularStringField(value: &_storage._memoryOptimizerTargetNodeNameScope) }()
        case 7: try { try decoder.decodeSingularEnumField(value: &_storage._arithmeticOptimization) }()
        case 8: try { try decoder.decodeSingularEnumField(value: &_storage._dependencyOptimization) }()
        case 9: try { try decoder.decodeSingularEnumField(value: &_storage._loopOptimization) }()
        case 10: try { try decoder.decodeSingularEnumField(value: &_storage._functionOptimization) }()
        case 11: try { try decoder.decodeSingularEnumField(value: &_storage._debugStripper) }()
        case 12: try { try decoder.decodeSingularEnumField(value: &_storage._metaOptimizerIterations) }()
        case 13: try { try decoder.decodeSingularEnumField(value: &_storage._shapeOptimization) }()
        case 14: try { try decoder.decodeSingularEnumField(value: &_storage._remapping) }()
        case 15: try { try decoder.decodeSingularEnumField(value: &_storage._scopedAllocatorOptimization) }()
        case 16: try { try decoder.decodeSingularMessageField(value: &_storage._scopedAllocatorOpts) }()
        case 17: try { try decoder.decodeSingularInt32Field(value: &_storage._minGraphNodes) }()
        case 18: try { try decoder.decodeSingularEnumField(value: &_storage._pinToHostOptimization) }()
        case 19: try { try decoder.decodeSingularBoolField(value: &_storage._disableMetaOptimizer) }()
        case 20: try { try decoder.decodeSingularInt64Field(value: &_storage._metaOptimizerTimeoutMs) }()
        case 21: try { try decoder.decodeSingularBoolField(value: &_storage._failOnOptimizerErrors) }()
        case 22: try { try decoder.decodeSingularEnumField(value: &_storage._implementationSelector) }()
        case 23: try { try decoder.decodeSingularEnumField(value: &_storage._autoMixedPrecision) }()
        case 24: try { try decoder.decodeSingularEnumField(value: &_storage._commonSubgraphElimination) }()
        case 25: try { try decoder.decodeSingularEnumField(value: &_storage._autoMixedPrecisionMkl) }()
        case 26: try { try decoder.decodeSingularBoolField(value: &_storage._experimentalDisableCompressedTensorOptimization) }()
        case 27: try { try decoder.decodeSingularBoolField(value: &_storage._experimentalDisableFoldingQuantizationEmulation) }()
        case 28: try { try decoder.decodeSingularEnumField(value: &_storage._usePluginOptimizers) }()
        case 29: try { try decoder.decodeSingularEnumField(value: &_storage._autoMixedPrecisionCpu) }()
        case 50: try { try decoder.decodeSingularEnumField(value: &_storage._cpuLayoutConversion) }()
        case 100: try { try decoder.decodeRepeatedStringField(value: &_storage._optimizers) }()
        case 200: try { try decoder.decodeRepeatedMessageField(value: &_storage._customOptimizers) }()
        case 300: try { try decoder.decodeSingularMessageField(value: &_storage._interOptimizerVerifierConfig) }()
        case 301: try { try decoder.decodeSingularMessageField(value: &_storage._postOptimizationVerifierConfig) }()
        default: break
        }
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    try withExtendedLifetime(_storage) { (_storage: _StorageClass) in
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every if/case branch local when no optimizations
      // are enabled. https://github.com/apple/swift-protobuf/issues/1034 and
      // https://github.com/apple/swift-protobuf/issues/1182
      if _storage._layoutOptimizer != .default {
        try visitor.visitSingularEnumField(value: _storage._layoutOptimizer, fieldNumber: 1)
      }
      if _storage._disableModelPruning != false {
        try visitor.visitSingularBoolField(value: _storage._disableModelPruning, fieldNumber: 2)
      }
      if _storage._constantFolding != .default {
        try visitor.visitSingularEnumField(value: _storage._constantFolding, fieldNumber: 3)
      }
      if _storage._memoryOptimization != .defaultMemOpt {
        try visitor.visitSingularEnumField(value: _storage._memoryOptimization, fieldNumber: 4)
      }
      try { if let v = _storage._autoParallel {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 5)
      } }()
      if !_storage._memoryOptimizerTargetNodeNameScope.isEmpty {
        try visitor.visitSingularStringField(value: _storage._memoryOptimizerTargetNodeNameScope, fieldNumber: 6)
      }
      if _storage._arithmeticOptimization != .default {
        try visitor.visitSingularEnumField(value: _storage._arithmeticOptimization, fieldNumber: 7)
      }
      if _storage._dependencyOptimization != .default {
        try visitor.visitSingularEnumField(value: _storage._dependencyOptimization, fieldNumber: 8)
      }
      if _storage._loopOptimization != .default {
        try visitor.visitSingularEnumField(value: _storage._loopOptimization, fieldNumber: 9)
      }
      if _storage._functionOptimization != .default {
        try visitor.visitSingularEnumField(value: _storage._functionOptimization, fieldNumber: 10)
      }
      if _storage._debugStripper != .default {
        try visitor.visitSingularEnumField(value: _storage._debugStripper, fieldNumber: 11)
      }
      if _storage._metaOptimizerIterations != .defaultNumIters {
        try visitor.visitSingularEnumField(value: _storage._metaOptimizerIterations, fieldNumber: 12)
      }
      if _storage._shapeOptimization != .default {
        try visitor.visitSingularEnumField(value: _storage._shapeOptimization, fieldNumber: 13)
      }
      if _storage._remapping != .default {
        try visitor.visitSingularEnumField(value: _storage._remapping, fieldNumber: 14)
      }
      if _storage._scopedAllocatorOptimization != .default {
        try visitor.visitSingularEnumField(value: _storage._scopedAllocatorOptimization, fieldNumber: 15)
      }
      try { if let v = _storage._scopedAllocatorOpts {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 16)
      } }()
      if _storage._minGraphNodes != 0 {
        try visitor.visitSingularInt32Field(value: _storage._minGraphNodes, fieldNumber: 17)
      }
      if _storage._pinToHostOptimization != .default {
        try visitor.visitSingularEnumField(value: _storage._pinToHostOptimization, fieldNumber: 18)
      }
      if _storage._disableMetaOptimizer != false {
        try visitor.visitSingularBoolField(value: _storage._disableMetaOptimizer, fieldNumber: 19)
      }
      if _storage._metaOptimizerTimeoutMs != 0 {
        try visitor.visitSingularInt64Field(value: _storage._metaOptimizerTimeoutMs, fieldNumber: 20)
      }
      if _storage._failOnOptimizerErrors != false {
        try visitor.visitSingularBoolField(value: _storage._failOnOptimizerErrors, fieldNumber: 21)
      }
      if _storage._implementationSelector != .default {
        try visitor.visitSingularEnumField(value: _storage._implementationSelector, fieldNumber: 22)
      }
      if _storage._autoMixedPrecision != .default {
        try visitor.visitSingularEnumField(value: _storage._autoMixedPrecision, fieldNumber: 23)
      }
      if _storage._commonSubgraphElimination != .default {
        try visitor.visitSingularEnumField(value: _storage._commonSubgraphElimination, fieldNumber: 24)
      }
      if _storage._autoMixedPrecisionMkl != .default {
        try visitor.visitSingularEnumField(value: _storage._autoMixedPrecisionMkl, fieldNumber: 25)
      }
      if _storage._experimentalDisableCompressedTensorOptimization != false {
        try visitor.visitSingularBoolField(value: _storage._experimentalDisableCompressedTensorOptimization, fieldNumber: 26)
      }
      if _storage._experimentalDisableFoldingQuantizationEmulation != false {
        try visitor.visitSingularBoolField(value: _storage._experimentalDisableFoldingQuantizationEmulation, fieldNumber: 27)
      }
      if _storage._usePluginOptimizers != .default {
        try visitor.visitSingularEnumField(value: _storage._usePluginOptimizers, fieldNumber: 28)
      }
      if _storage._autoMixedPrecisionCpu != .default {
        try visitor.visitSingularEnumField(value: _storage._autoMixedPrecisionCpu, fieldNumber: 29)
      }
      if _storage._cpuLayoutConversion != .noConversionOnCpu {
        try visitor.visitSingularEnumField(value: _storage._cpuLayoutConversion, fieldNumber: 50)
      }
      if !_storage._optimizers.isEmpty {
        try visitor.visitRepeatedStringField(value: _storage._optimizers, fieldNumber: 100)
      }
      if !_storage._customOptimizers.isEmpty {
        try visitor.visitRepeatedMessageField(value: _storage._customOptimizers, fieldNumber: 200)
      }
      try { if let v = _storage._interOptimizerVerifierConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 300)
      } }()
      try { if let v = _storage._postOptimizationVerifierConfig {
        try visitor.visitSingularMessageField(value: v, fieldNumber: 301)
      } }()
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_RewriterConfig, rhs: Tensorboard_RewriterConfig) -> Bool {
    if lhs._storage !== rhs._storage {
      let storagesAreEqual: Bool = withExtendedLifetime((lhs._storage, rhs._storage)) { (_args: (_StorageClass, _StorageClass)) in
        let _storage = _args.0
        let rhs_storage = _args.1
        if _storage._cpuLayoutConversion != rhs_storage._cpuLayoutConversion {return false}
        if _storage._layoutOptimizer != rhs_storage._layoutOptimizer {return false}
        if _storage._constantFolding != rhs_storage._constantFolding {return false}
        if _storage._shapeOptimization != rhs_storage._shapeOptimization {return false}
        if _storage._remapping != rhs_storage._remapping {return false}
        if _storage._commonSubgraphElimination != rhs_storage._commonSubgraphElimination {return false}
        if _storage._arithmeticOptimization != rhs_storage._arithmeticOptimization {return false}
        if _storage._dependencyOptimization != rhs_storage._dependencyOptimization {return false}
        if _storage._loopOptimization != rhs_storage._loopOptimization {return false}
        if _storage._functionOptimization != rhs_storage._functionOptimization {return false}
        if _storage._debugStripper != rhs_storage._debugStripper {return false}
        if _storage._disableModelPruning != rhs_storage._disableModelPruning {return false}
        if _storage._scopedAllocatorOptimization != rhs_storage._scopedAllocatorOptimization {return false}
        if _storage._pinToHostOptimization != rhs_storage._pinToHostOptimization {return false}
        if _storage._implementationSelector != rhs_storage._implementationSelector {return false}
        if _storage._autoMixedPrecision != rhs_storage._autoMixedPrecision {return false}
        if _storage._autoMixedPrecisionMkl != rhs_storage._autoMixedPrecisionMkl {return false}
        if _storage._autoMixedPrecisionCpu != rhs_storage._autoMixedPrecisionCpu {return false}
        if _storage._disableMetaOptimizer != rhs_storage._disableMetaOptimizer {return false}
        if _storage._usePluginOptimizers != rhs_storage._usePluginOptimizers {return false}
        if _storage._metaOptimizerIterations != rhs_storage._metaOptimizerIterations {return false}
        if _storage._minGraphNodes != rhs_storage._minGraphNodes {return false}
        if _storage._experimentalDisableCompressedTensorOptimization != rhs_storage._experimentalDisableCompressedTensorOptimization {return false}
        if _storage._experimentalDisableFoldingQuantizationEmulation != rhs_storage._experimentalDisableFoldingQuantizationEmulation {return false}
        if _storage._memoryOptimization != rhs_storage._memoryOptimization {return false}
        if _storage._memoryOptimizerTargetNodeNameScope != rhs_storage._memoryOptimizerTargetNodeNameScope {return false}
        if _storage._metaOptimizerTimeoutMs != rhs_storage._metaOptimizerTimeoutMs {return false}
        if _storage._autoParallel != rhs_storage._autoParallel {return false}
        if _storage._failOnOptimizerErrors != rhs_storage._failOnOptimizerErrors {return false}
        if _storage._scopedAllocatorOpts != rhs_storage._scopedAllocatorOpts {return false}
        if _storage._optimizers != rhs_storage._optimizers {return false}
        if _storage._customOptimizers != rhs_storage._customOptimizers {return false}
        if _storage._interOptimizerVerifierConfig != rhs_storage._interOptimizerVerifierConfig {return false}
        if _storage._postOptimizationVerifierConfig != rhs_storage._postOptimizationVerifierConfig {return false}
        return true
      }
      if !storagesAreEqual {return false}
    }
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}

extension Tensorboard_RewriterConfig.Toggle: SwiftProtobuf._ProtoNameProviding {
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "DEFAULT"),
    1: .same(proto: "ON"),
    2: .same(proto: "OFF"),
    3: .same(proto: "AGGRESSIVE"),
    4: .same(proto: "EXPERIMENTAL_MLIR"),
    5: .same(proto: "EXPERIMENTAL_BOTH"),
  ]
}

extension Tensorboard_RewriterConfig.CpuLayout: SwiftProtobuf._ProtoNameProviding {
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "NO_CONVERSION_ON_CPU"),
    1: .same(proto: "NCHW_TO_NHWC"),
    2: .same(proto: "NHWC_TO_NCHW"),
  ]
}

extension Tensorboard_RewriterConfig.NumIterationsType: SwiftProtobuf._ProtoNameProviding {
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "DEFAULT_NUM_ITERS"),
    1: .same(proto: "ONE"),
    2: .same(proto: "TWO"),
  ]
}

extension Tensorboard_RewriterConfig.MemOptType: SwiftProtobuf._ProtoNameProviding {
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    0: .same(proto: "DEFAULT_MEM_OPT"),
    1: .same(proto: "NO_MEM_OPT"),
    2: .same(proto: "MANUAL"),
    3: .same(proto: "HEURISTICS"),
    4: .same(proto: "SWAPPING_HEURISTICS"),
    5: .same(proto: "RECOMPUTATION_HEURISTICS"),
    6: .same(proto: "SCHEDULING_HEURISTICS"),
  ]
}

extension Tensorboard_RewriterConfig.CustomGraphOptimizer: SwiftProtobuf.Message, SwiftProtobuf._MessageImplementationBase, SwiftProtobuf._ProtoNameProviding {
  static let protoMessageName: String = Tensorboard_RewriterConfig.protoMessageName + ".CustomGraphOptimizer"
  static let _protobuf_nameMap: SwiftProtobuf._NameMap = [
    1: .same(proto: "name"),
    2: .standard(proto: "parameter_map"),
  ]

  mutating func decodeMessage<D: SwiftProtobuf.Decoder>(decoder: inout D) throws {
    while let fieldNumber = try decoder.nextFieldNumber() {
      // The use of inline closures is to circumvent an issue where the compiler
      // allocates stack space for every case branch when no optimizations are
      // enabled. https://github.com/apple/swift-protobuf/issues/1034
      switch fieldNumber {
      case 1: try { try decoder.decodeSingularStringField(value: &self.name) }()
      case 2: try { try decoder.decodeMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Tensorboard_AttrValue>.self, value: &self.parameterMap) }()
      default: break
      }
    }
  }

  func traverse<V: SwiftProtobuf.Visitor>(visitor: inout V) throws {
    if !self.name.isEmpty {
      try visitor.visitSingularStringField(value: self.name, fieldNumber: 1)
    }
    if !self.parameterMap.isEmpty {
      try visitor.visitMapField(fieldType: SwiftProtobuf._ProtobufMessageMap<SwiftProtobuf.ProtobufString,Tensorboard_AttrValue>.self, value: self.parameterMap, fieldNumber: 2)
    }
    try unknownFields.traverse(visitor: &visitor)
  }

  static func ==(lhs: Tensorboard_RewriterConfig.CustomGraphOptimizer, rhs: Tensorboard_RewriterConfig.CustomGraphOptimizer) -> Bool {
    if lhs.name != rhs.name {return false}
    if lhs.parameterMap != rhs.parameterMap {return false}
    if lhs.unknownFields != rhs.unknownFields {return false}
    return true
  }
}
